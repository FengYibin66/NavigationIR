{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:31:01.265295Z",
     "start_time": "2024-05-31T13:31:01.181461Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import requests, io\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e552ffaf1410a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:30:12.879492Z",
     "start_time": "2024-05-31T13:30:12.666092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638385f9-691d-4d63-8928-66589f25279b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    model = blip_itm(pretrained='./BLIP/chatir_weights.ckpt',  # Download from Google Drive, see README.md\n",
    "                     med_config='BLIP/configs/med_config.json',\n",
    "                     image_size=224,\n",
    "                     vit='base'\n",
    "                     )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # define Image Embedder (raw_image --> img_feature)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    # define dialog encoder (dialog --> img_feature)\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\"\n",
    "                               ).to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text strings.\n",
    "            processor (transformers processor): Processor to tokenize the text.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        # self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # Get the text at the provided index\n",
    "        return {'text': text}\n",
    "\n",
    "\n",
    "\n",
    "def encode_text(dataset, model):\n",
    "    \"\"\"CLIP for encode text \"\"\"\n",
    "    # model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=100,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=4,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False,\n",
    "                                              \n",
    "                                              )\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = model(batch['text'])\n",
    "            all_features.append(features.cpu())  \n",
    "    return torch.cat(all_features)  \n",
    "\n",
    "def retrieve_topk_images(query: list,\n",
    "                         topk=10,\n",
    "                         faiss_model=None,\n",
    "                         text_encoder=None,\n",
    "                         id2image=None,\n",
    "                         ):\n",
    "    text_dataset = TextDataset(query)\n",
    "    query_vec = encode_text(text_dataset, text_encoder)\n",
    "    query_vec = query_vec.numpy()\n",
    "    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "\n",
    "    distance, indices = faiss_model.search(query_vec, topk)\n",
    "    indices = np.array(indices)\n",
    "    image_paths = [[id2image.get(idx, 'path/not/found') for idx in row] for row in indices]\n",
    "    return image_paths, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a980-83ab-4457-85e8-1e8488b58ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('recall_res.csv')\n",
    "faiss_model = faiss.read_index('./checkpoints/blip_faiss.index')\n",
    "with open('./checkpoints/id2image.pickle', 'rb') as f:\n",
    "    id2image = pickle.load(f)\n",
    "    \n",
    "with open('./checkpoints/blip_image_embedding.pickle', 'rb') as f:\n",
    "    image_vector = pickle.load(f)\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf677c9c-9032-40ce-bcbf-41ce3352c2ba",
   "metadata": {},
   "source": [
    "### 2. load llava question model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824bbcee-3144-44fb-98be-fb364bf39587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from safetensors import safe_open\n",
    "from pllava import PllavaProcessor, PllavaForConditionalGeneration, PllavaConfig\n",
    "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_in_model\n",
    "from accelerate.utils import get_balanced_memory\n",
    "\n",
    "\n",
    "def load_pllava(repo_id, num_frames, use_lora=False, weight_dir=None, lora_alpha=32, use_multi_gpus=False, pooling_shape=(16,12,12)):\n",
    "    kwargs = {\n",
    "        'num_frames': num_frames,\n",
    "    }\n",
    "    # print(\"===============>pooling_shape\", pooling_shape)\n",
    "    if num_frames == 0:\n",
    "        kwargs.update(pooling_shape=(0,12,12)) # produce a bug if ever usen the pooling projector\n",
    "    config = PllavaConfig.from_pretrained(\n",
    "        repo_id if not use_lora else weight_dir,\n",
    "        pooling_shape=pooling_shape,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model = PllavaForConditionalGeneration.from_pretrained(repo_id, config=config, torch_dtype=torch.bfloat16)\n",
    "        \n",
    "    try:\n",
    "        processor = PllavaProcessor.from_pretrained(repo_id)\n",
    "    except Exception as e:\n",
    "        processor = PllavaProcessor.from_pretrained('llava-hf/llava-1.5-7b-hf')\n",
    "\n",
    "    # config lora\n",
    "    if use_lora and weight_dir is not None:\n",
    "        print(\"Use lora\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, inference_mode=False,  target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            r=128, lora_alpha=lora_alpha, lora_dropout=0.\n",
    "        )\n",
    "        print(\"Lora Scaling:\", lora_alpha/128)\n",
    "        model.language_model = get_peft_model(model.language_model, peft_config)\n",
    "        assert weight_dir is not None, \"pass a folder to your lora weight\"\n",
    "        print(\"Finish use lora\")\n",
    "    \n",
    "    # load weights\n",
    "    if weight_dir is not None:\n",
    "        state_dict = {}\n",
    "        save_fnames = os.listdir(weight_dir)\n",
    "        if \"model.safetensors\" in save_fnames:\n",
    "            use_full = False\n",
    "            for fn in save_fnames:\n",
    "                if fn.startswith('model-0'):\n",
    "                    use_full=True        \n",
    "                    break\n",
    "        else:\n",
    "            use_full= True\n",
    "\n",
    "        if not use_full:\n",
    "            print(\"Loading weight from\", weight_dir, \"model.safetensors\")\n",
    "            with safe_open(f\"{weight_dir}/model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "                for k in f.keys():\n",
    "                    state_dict[k] = f.get_tensor(k)\n",
    "        else:\n",
    "            print(\"Loading weight from\", weight_dir)\n",
    "            for fn in save_fnames:\n",
    "                if fn.startswith('model-0'):\n",
    "                    with safe_open(f\"{weight_dir}/{fn}\", framework=\"pt\", device=\"cpu\") as f:\n",
    "                        for k in f.keys():\n",
    "                            state_dict[k] = f.get_tensor(k)\n",
    "            \n",
    "        if 'model' in state_dict.keys():\n",
    "            msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "        else:\n",
    "            msg = model.load_state_dict(state_dict, strict=False)\n",
    "        print(msg)\n",
    "    # dispatch model weight\n",
    "    if use_multi_gpus:\n",
    "        max_memory = get_balanced_memory(\n",
    "            model,\n",
    "            max_memory=None,\n",
    "            no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "            dtype='bfloat16',\n",
    "            low_zero=False,\n",
    "        )\n",
    "\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            max_memory=max_memory,\n",
    "            no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "            dtype='bfloat16'\n",
    "        )\n",
    "\n",
    "        dispatch_model(model, device_map=device_map)\n",
    "        print(model.hf_device_map)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33056982-de52-41a9-988b-374b4d61e548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "llava_model, llava_processor = load_pllava(repo_id='/root/autodl-tmp/.autodl/HYF/questionIR/CSS/MODELS/pllava-7b', #'llava-hf/llava-1.5-7b-hf', \n",
    "            num_frames=5, # num_images = 5\n",
    "            use_lora=True, \n",
    "            weight_dir='/root/autodl-tmp/.autodl/HYF/questionIR/CSS/MODELS/pllava-7b', \n",
    "            lora_alpha=4, \n",
    "            use_multi_gpus=False, \n",
    "            pooling_shape=(5,12,12)\n",
    "            )\n",
    "llava_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f74f6c0-7783-41c9-808d-4e935c154626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def generate_question(option: str, \n",
    "                     images_path: list,\n",
    "                     model=None,\n",
    "                     processor=None):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "       option: list: user's description \n",
    "       images_path: list: retrieve images based on the option via BLIP\n",
    "       model: llave model\n",
    "       processor: llave processor\n",
    "       \n",
    "    Return:\n",
    "        return the question\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"You are Pllava, a large vision-language assistant. \n",
    "    You are able to understand the video content that the user provides, and assist the user with a variety of tasks using natural language.\n",
    "    Follow the instructions carefully and explain your answers in detail based on the provided video.\n",
    "    USER:<image>  USER: You need to find a common object that appears in all 5 pictures but has distinguishing features. Based on this object, ask a question to differentiate the pictures.\n",
    "\n",
    "                Remember, you must ensure the question is specific, not abstract, and the answer should be directly obtainable by looking at the images.\n",
    "\n",
    "                For example:\n",
    "                Example 1: All 5 pictures have people, but the number of people differs. You can ask about the number of people.\n",
    "                Example 2: All 5 pictures have cats, but the colors are different. You can ask about the color.\n",
    "                Example 3: All 5 pictures have traffic lights, but their positions differ. You can ask about the position of the traffic lights.\n",
    "\n",
    "                Ask a specific question based on the object that will help distinguish the pictures. The question is not overlapped with the description: {option}.\n",
    "                Don't ask 2 questions each time. such as what is the attribute of a or b\n",
    "\n",
    "                Output as the following format\n",
    "                {{\n",
    "                \"What is the common object that appears in all five pictures\":\"\",\n",
    "                \"What is he distinguishing feature that can help differentiate the picture\":\"\",\n",
    "                \"Question to differentiate the pictures\":\"\"\n",
    "                }}\n",
    "                \"\"\n",
    "                ASSISTANT:\n",
    "                \"\"\"\n",
    "    \n",
    "    \n",
    "    image_tensor = [load_image(img_file) for img_file in images_path]\n",
    "    inputs = processor(prompt, image_tensor, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(\"cuda\") for k,v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_token = model.generate(**inputs, media_type='video',\n",
    "                                    do_sample=False,\n",
    "                                    max_new_tokens=500, \n",
    "                                      num_beams=1, \n",
    "                                      min_length=1, \n",
    "                                    top_p=0.9, \n",
    "                                      repetition_penalty=1, \n",
    "                                      length_penalty=1, \n",
    "                                      temperature=1,\n",
    "                                    ) # dont need to long for the choice.\n",
    "    torch.cuda.empty_cache() # clear the history for this batch\n",
    "    output_text = processor.batch_decode(output_token, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    # extract the question \n",
    "    output_text = output_text.split('ASSISTANT:')[-1].strip()\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d58eb555-718b-44aa-a375-da2096715cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    pattern = r'{.*}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'parse incorrectly'\n",
    "\n",
    "def extract_output(text):\n",
    "    question_fewshot_json=json.loads(extract_json(text))\n",
    "    question=question_fewshot_json[\"Question to differentiate the pictures\"]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72f716-dd70-4901-9fc2-bf35503e3f60",
   "metadata": {},
   "source": [
    "### 3. Downsample retrieved image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c57826d-0003-44c8-aeac-b042897cfd97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def downsample_retrieved_images(images_path:list, strategy='clustering', image_vector=None, retrieve_image_indices:list=None):\n",
    "    if strategy == 'clustering':\n",
    "        topk_embedding = image_vector[retrieve_image_indices] # [batch_size, topk, embed_dim]\n",
    "        \n",
    "        kmeans = faiss.Kmeans(d=topk_embedding.shape[1], k=5, niter=10, verbose=False) # k=num_clusters, niter=epoch\n",
    "        # Train the KMeans object\n",
    "        kmeans.train(topk_embedding)\n",
    "        distance, clustering_label = kmeans.index.search(topk_embedding, 1)\n",
    "        \n",
    "        clustering_label = clustering_label.flatten()\n",
    "        label_to_paths = defaultdict(list)\n",
    "        for path, label in zip(images_path, clustering_label):\n",
    "            label_to_paths[label].append(path)\n",
    "        \n",
    "        sampled_paths_list = [random.choice(paths) for paths in label_to_paths.values()]\n",
    "    elif strategy == 'interval':\n",
    "        sampled_paths_list = []\n",
    "        for i in range(0, 50, 10):\n",
    "            sampled_path = random.choice(images_path[i:i+10])\n",
    "            sampled_paths_list.append(sampled_path)\n",
    "    elif strategy == 'topk_cos_similiarity':\n",
    "        sampled_paths_list = images_path\n",
    "    return sampled_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca8551d3-8ed4-458f-9ec9-605e0ea032b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv('recall_res.csv')\n",
    "# option_list = data['option'].tolist()\n",
    "# # option_list\n",
    "\n",
    "# non-top10 dataset\n",
    "\n",
    "recall_res_raw = pd.DataFrame()\n",
    "for p in os.listdir('./top_interval_res'):\n",
    "    if '0-9' not in p:\n",
    "        tmp = pd.read_csv('./top_interval_res/' + p)\n",
    "        recall_res_raw = pd.concat([recall_res_raw, tmp], axis=0)\n",
    "recall_res_raw = recall_res_raw[['option', 'recall_images', 'target_image', 'target_image_position']]\n",
    "recall_res = recall_res_raw.reset_index()\n",
    "option_list = recall_res['option'].tolist()\n",
    "# option_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71238c6-8da4-4a95-87bc-22eacb8fdf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# topk = 100 and strategy='clustering' --> Kmeans labeled samples\n",
    "topk = 50 # and strategy='clustering' --> interval sampled\n",
    "# topk = 5 # topk sample\n",
    "\n",
    "\n",
    "one_tune_questions = []\n",
    "downsampled_images_paths = []\n",
    "top100_images_path = []\n",
    "for opt in tqdm(option_list):\n",
    "    # the type of query hyperparameter in retrieve_topk_images is list\n",
    "    images_path, indices = retrieve_topk_images([opt],\n",
    "                                             topk=topk,\n",
    "                                             faiss_model=faiss_model,\n",
    "                                             text_encoder=dialog_encoder,\n",
    "                                             id2image=id2image,\n",
    "                             )\n",
    "    images_path, indices = images_path[0], indices[0] \n",
    "    top100_images_path.append(images_path)\n",
    "    # Downsample retrieved image\n",
    "    downsampled_images_path = downsample_retrieved_images(images_path, \n",
    "                                                       strategy='interval', \n",
    "                                                       image_vector=image_vector, \n",
    "                                                       retrieve_image_indices=indices\n",
    "                                                      )\n",
    "    # print(downsampled_images_path)\n",
    "    downsampled_images_paths.append(downsampled_images_path)\n",
    "    # the type of option hyperparameter in generate_question is str\n",
    "    output_ori = generate_question(opt, \n",
    "                                         downsampled_images_path,\n",
    "                                         model=llava_model,\n",
    "                                         processor=llava_processor\n",
    "                                       )\n",
    "    # extract output\n",
    "    try:\n",
    "        output_question = extract_output(output_ori)\n",
    "    except:\n",
    "        output_question = ''\n",
    "        \n",
    "    one_tune_questions.append(output_question)\n",
    "    \n",
    "    result = {\n",
    "            \"option\": opt,\n",
    "            \"top100_images_path\": images_path,\n",
    "            \"downsampled_images_paths\": downsampled_images_path,\n",
    "            \"one_tune_questions\": output_question,\n",
    "            \"output_ori\": output_ori\n",
    "        }\n",
    "    with open('./experiment_res/interval_prompt_without_option_newprompt_13b.jsonl', 'a') as f:  # 'a' for append mode\n",
    "        json.dump(result, f)\n",
    "        f.write(\"\\n\")  # Add newline to separate entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d8e25-d4ea-4491-a890-44070eb032d9",
   "metadata": {},
   "source": [
    "### 5. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14673a52-e8df-40ca-94fa-8982e502b33a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple version\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, option, images_path, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of dict): List of data dictionaries, each containing images and conversations.\n",
    "            processor: A processor for the model.\n",
    "        \"\"\"\n",
    "        self.option = option\n",
    "        self.images_path = images_path\n",
    "        self.processor = processor\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_paths = self.images_path[idx] # 5 images per batch\n",
    "        option = self.option[idx]\n",
    "\n",
    "        # Load images\n",
    "        image_tensor = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "        prompt = f\"\"\"You are Pllava, a large vision-language assistant. \n",
    "    You are able to understand the video content that the user provides, and assist the user with a variety of tasks using natural language.\n",
    "    Follow the instructions carefully and explain your answers in detail based on the provided video.\n",
    "     USER:<image>  USER: our task is to identify the target image based on descriptions provided by users. Due to the ambiguity in user descriptions, you have already searched and found 5 images based on these descriptions. \n",
    "     Your task is to analyze the content of these 5 images, ask a question to clarify the user's needs, and your question is not overlapping with the descriptions. Thereby helping the user quickly find the target image. You only need to output one questions within 30 words.\n",
    "                Complete the following tasks step by step:\n",
    "                1. Combine the textual description, observe these 5 images, and analyze and summarize their common points and differences.\n",
    "                2. To find the target image, ask a question based on these differences that can clarify the user’s needs and help them quickly find the target image.\n",
    "                [Your Question]\n",
    "                Based on the images and sentence description, {option}. One question you asked is:\n",
    "     ASSISTANT:\"\"\"\n",
    "\n",
    "  \n",
    "        encode = self.processor(prompt, image_tensor, return_tensors=\"pt\")\n",
    "\n",
    "        return {'input_ids': encode['input_ids'].squeeze(0), # shape: [seq_len]\n",
    "                'attention_mask': encode['attention_mask'].squeeze(0), # shape: [seq_len]\n",
    "                'pixel_values': encode['pixel_values'], # shape: [num_images, 3, 224, 224]\n",
    "                }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle the merging of text and image data.\n",
    "    Args:\n",
    "        batch: A list of tuples (input_ids, pixel_values) from the dataset.\n",
    "    Returns:\n",
    "        A tuple (input_ids, pixel_values) where:\n",
    "            input_ids: Tensor of shape [batch_size, seq_len]\n",
    "            pixel_values: Tensor of shape [batch_size * num_images, 3, 224, 224]\n",
    "    \"\"\"\n",
    "    # print(batch.keys())\n",
    "    # input_ids, attention_mask, pixel_values = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item['input_ids'] for item in batch],\n",
    "                                                batch_first=True,\n",
    "                                                padding_value=llava_processor.tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    pixel_values = torch.cat([item['pixel_values'] for item in batch], dim=0)  # flatten images\n",
    "\n",
    "    return {'input_ids': input_ids, # shape: [batch_size, seq_len]\n",
    "            'attention_mask': attention_mask, # shape: [batch_size, seq_len]\n",
    "            'pixel_values': pixel_values, # shape: [batch_size * num_images, 3, 224, 224]\n",
    "            }\n",
    "\n",
    "# load temp json file\n",
    "def generate_question_batch(option: list, \n",
    "                     images_path: list[list],\n",
    "                     model=None,\n",
    "                     processor=None):\n",
    "    dataset = MultiModalDataset(data, processor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                             batch_size=3,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True,\n",
    "                                             num_workers=4,\n",
    "                                            )\n",
    "    output_res = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k:v.to(\"cuda\") for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            output_token = model.generate(**inputs, media_type='video',\n",
    "                                    do_sample=False,\n",
    "                                    max_new_tokens=500, \n",
    "                                      num_beams=1, \n",
    "                                      min_length=1, \n",
    "                                    top_p=0.9, \n",
    "                                      repetition_penalty=1, \n",
    "                                      length_penalty=1, \n",
    "                                      temperature=1,\n",
    "                                    ) # dont need to long for the choice.\n",
    "        output_text = processor.batch_decode(output_token, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        for text in output_text:\n",
    "            processed_text = text.split('ASSISTANT:')[-1].strip()\n",
    "            output_res.append(processed_text)\n",
    "    return output_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344f4852-9a6d-4bad-87cb-edcf3167f606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('recall_res.csv')\n",
    "option = data['option']\n",
    "# Batch Inference \n",
    "retrieved_images_path, retrieved_indices = retrieve_topk_images(option,\n",
    "                                             topk=100,\n",
    "                                             faiss_model=faiss_model,\n",
    "                                             text_encoder=dialog_encoder,\n",
    "                                             id2image=id2image,\n",
    "                             )\n",
    "\n",
    "# Downsample retrieved image\n",
    "downsampled_images_paths = []\n",
    "for img_path, idx in zip(retrieved_images_path, retrieved_indices):\n",
    "    downsampled_images_path = downsample_retrieved_images(img_path, \n",
    "                                                       strategy='interval', \n",
    "                                                       image_vector=image_vector, \n",
    "                                                       retrieve_image_indices=idx\n",
    "                                                      )\n",
    "    downsampled_images_paths.append(downsampled_images_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1fd7f9-f9ad-46ad-91ab-8ae6b23c46a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc31fb-4a62-4659-b561-799ed13022da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GENERATE QUestion\n",
    "one_tune_question = generate_question_batch(option,\n",
    "                       downsampled_images_paths,\n",
    "                        model=llava_model,\n",
    "                         processor=llava_processor\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ac3905-ead8-498f-932e-0605235705a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "query = [\n",
    "    'a women dressed in white playing tennis on a clay court',\n",
    "    'a white and gray cat perched on a door that is partially open',\n",
    "    'a bird sitting on a branch of a tree',\n",
    "    'a close up of a wine glass with a bartender in the background',\n",
    "    'a group of giraffes is standing in a savannah'\n",
    "]\n",
    "\n",
    "option = 'rows of wooden chairs and benches in a classroom'\n",
    "\n",
    "# the type of query hyperparameter in retrieve_topk_images is list\n",
    "images_path, indices = retrieve_topk_images([option],\n",
    "                         topk=100,\n",
    "                         faiss_model=faiss_model,\n",
    "                         text_encoder=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         )\n",
    "images_path, indices = images_path[0], indices[0]\n",
    "selected_images_path = downsample_retrieved_images(images_path, strategy='clustering', image_vector=image_vector, retrieve_image_indices=indices)\n",
    "# the type of option hyperparameter in generate_question is str\n",
    "output_question = generate_question(option, \n",
    "                     selected_images_path,\n",
    "                     model=llava_model,\n",
    "                     processor=llava_processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c6e55-4fdc-419b-90d8-4c329211e25b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462ce8a-8b1e-425f-93eb-8b101a3fbf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_image(images_path, option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1dcc8-6dc1-4aa8-8ccb-d3548fdecd66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = data['option'].tolist()\n",
    "query = [\n",
    "    'a women dressed in white playing tennis on a clay court',\n",
    "    # 'a white and gray cat perched on a door that is partially open',\n",
    "    # 'a bird sitting on a branch of a tree',\n",
    "    # 'a close up of a wine glass with a bartender in the background',\n",
    "    # 'a group of giraffes is standing in a savannah'\n",
    "]\n",
    "topk = 100\n",
    "\n",
    "image_paths, indices = retrieve_topk_images(query,\n",
    "                         topk=10,\n",
    "                         faiss_model=faiss_model,\n",
    "                         text_encoder=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5e4b1-1197-4f36-b730-e40e33bc0a23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d5931-c356-4ea4-80b5-212f5193d7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topk_embedding = image_vector[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f536585-48f9-4966-9a83-503aad5dba68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topk_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4801c86f98099f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:34:31.892294Z",
     "start_time": "2024-05-31T13:34:31.605846Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "kmeans = faiss.Kmeans(d=image_vector.shape[1], k=n_clusters, niter=10, verbose=True)\n",
    "# Train the KMeans object\n",
    "kmeans.train(topk_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa98dfec59fda7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:35:59.603935Z",
     "start_time": "2024-05-31T13:35:59.599261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the cluster centroids\n",
    "centroids = kmeans.centroids\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca962d2e3f43fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:38:20.104061Z",
     "start_time": "2024-05-31T13:38:20.087523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign the data points to the nearest cluster\n",
    "distance, clustering_label = kmeans.index.search(topk_embedding[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ebefd-5023-4428-a126-61d859aa53a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_paths, clustering_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5e5c96bd84e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:38:21.149476Z",
     "start_time": "2024-05-31T13:38:21.145167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9aa85d-36c3-461a-b21c-fc4260ff37cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "image_paths = [\n",
    "    './playground/data/css_data/unlabeled2017/000000446161.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000396918.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000282138.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000296263.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000208485.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000087032.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000382609.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000201751.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000307338.jpg',\n",
    "    './playground/data/css_data/unlabeled2017/000000239956.jpg'\n",
    "]\n",
    "clustering_label = np.array([[4],\n",
    "                              [0],\n",
    "                              [1],\n",
    "                              [0],\n",
    "                              [2],\n",
    "                              [4],\n",
    "                              [3],\n",
    "                              [0],\n",
    "                              [4],\n",
    "                              [3]])\n",
    "\n",
    "\n",
    "clustering_label = clustering_label.flatten()\n",
    "clustering_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b1ddba3-67bf-4894-8697-868997e3753a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T13:38:21.153348Z",
     "start_time": "2024-05-31T13:38:21.150795Z"
    },
    "tags": []
   },
   "source": [
    "展示聚类的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ea7df-1293-4bc1-a83e-bf6ab4c273dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_clusted_images(clustering_label, image_paths):\n",
    "    \n",
    "    images_by_label = {}\n",
    "\n",
    "    for label, path in zip(clustering_label, image_paths):\n",
    "        label = label[0]  \n",
    "        if label not in images_by_label:\n",
    "            images_by_label[label] = []\n",
    "        images_by_label[label].append(path)\n",
    "\n",
    "\n",
    "    for label, paths in images_by_label.items():\n",
    "        num_images = len(paths)\n",
    "        cols = 6  \n",
    "        rows = int(np.ceil(num_images / cols) ) \n",
    "        fig, axs = plt.subplots(rows, cols, figsize=(20, 5 * rows))  \n",
    "        fig.suptitle(f'Label {label}', fontsize=16)\n",
    "\n",
    "\n",
    "        axs = axs.flatten()\n",
    "\n",
    "\n",
    "        for ax, path in zip(axs, paths):\n",
    "            img = Image.open(path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')  \n",
    "\n",
    "\n",
    "        for ax in axs[len(paths):]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4f4b9-0c29-4e30-a94e-c82743105b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters=5\n",
    "for tk in range(topk_embedding.shape[0]):\n",
    "    print('Query: ', query[tk])\n",
    "    kmeans = faiss.Kmeans(d=image_vector.shape[1], k=n_clusters, niter=10, verbose=True)\n",
    "    kmeans.train(topk_embedding[tk])\n",
    "    distance, clustering_label = kmeans.index.search(topk_embedding[tk], 1)\n",
    "    clustering_label_image_path = image_paths[tk]\n",
    "    show_clusted_images(clustering_label, clustering_label_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb7e6967-313b-4334-9149-4d95b4f07bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('recall_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09670260-2384-4dd5-bb0f-460638ae6f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_repeat_5times = data.loc[data.index.repeat(5)].reset_index(drop=True)\n",
    "data_repeat_5times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a1e5d2-060b-47cf-9013-ebbf785aea06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import json\n",
    "def extract_json(text):\n",
    "    pattern = r'{.*}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'parse incorrectly'\n",
    "\n",
    "def extract_llm_ouptut(text):\n",
    "    ext_json = extract_json(text)\n",
    "    try:\n",
    "        question = json.loads(ext_json)['Question to differentiate the pictures']\n",
    "    except:\n",
    "        question = ''\n",
    "        pass\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ae35d-b9d0-48ee-9fec-469843c38492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpm= \"{\\n                  \\\"What is the common object that appears in all five pictures\\\": \\\"Book\\\",\\n                  \\\"What is the distinguishing feature that can help differentiate the picture\\\": \\\"The color of the book cover\\\",\\n                  \\\"Question to differentiate the pictures\\\": \\\"What color is the book cover in each of the five pictures?\\\"\\n                  }\"\n",
    "extract_llm_ouptut(tpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb58b0-5f48-4a24-adf6-e85adf4b4faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return [json.loads(l.strip(\"\\n\")) for l in f.readlines()]\n",
    "\n",
    "load_jsonl('./experiment_res/one_option_five_images_five_response_13b.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_questionIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
