{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb37801-9a24-482a-aefd-96f37428381e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:24.395729Z",
     "start_time": "2024-07-02T12:00:24.374258Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            data.append(json_object)\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = './experiment_res/interval_prompt_without_option_newprompt_13b.jsonl'\n",
    "jsonl_data = read_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05407704-5df9-4790-957c-271e45b344d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:25.685277Z",
     "start_time": "2024-07-02T12:00:25.679841Z"
    }
   },
   "outputs": [],
   "source": [
    "jsonl_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc301f29-b07a-4b03-9ba1-7dda12e72b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:29.501062Z",
     "start_time": "2024-07-02T12:00:26.981389Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c09dcb2-80cb-4a30-a2ac-fbe95132aa65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:29.513694Z",
     "start_time": "2024-07-02T12:00:29.502254Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_image(image_paths, sentence=None):\n",
    "    k=(len(image_paths)+4)//5\n",
    "    fig, axs = plt.subplots(nrows=k, ncols=5, figsize=(20, 8))  \n",
    "    axs = axs.flatten()  \n",
    "\n",
    "    \n",
    "    for ax, img_path in zip(axs, image_paths):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')  \n",
    "            ax.set_title(img_path.split('/')[-1])  \n",
    "        except FileNotFoundError:\n",
    "            ax.imshow(np.zeros((10, 10, 3), dtype=int))  \n",
    "            ax.axis('off')\n",
    "            ax.set_title('File Not Found')\n",
    "    if sentence:\n",
    "        fig.suptitle(sentence, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    model = blip_itm(pretrained='./BLIP/chatir_weights.ckpt',  # Download from Google Drive, see README.md\n",
    "                     med_config='BLIP/configs/med_config.json',\n",
    "                     image_size=224,\n",
    "                     vit='base'\n",
    "                     )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    # define dialog encoder (dialog --> img_feature)\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\"\n",
    "                               ).to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text strings.\n",
    "            processor (transformers processor): Processor to tokenize the text.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        # self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # Get the text at the provided index\n",
    "        return {'text': text}\n",
    "\n",
    "def encode_text(dataset, model):\n",
    "    \"\"\"CLIP for encode text \"\"\"\n",
    "    # model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=20,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=1,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False,\n",
    "                                              \n",
    "                                              )\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = model(batch['text'])\n",
    "            all_features.append(features.cpu())  \n",
    "    return torch.cat(all_features)  \n",
    "\n",
    "def retrieve_topk_images(query: list,\n",
    "                         topk=10,\n",
    "                         faiss_model=None,\n",
    "                         blip_model=None,\n",
    "                         id2image=None,\n",
    "                         processor=None, ):\n",
    "    text_dataset = TextDataset(query)\n",
    "    query_vec = encode_text(text_dataset, blip_model)\n",
    "    query_vec = query_vec.numpy()\n",
    "    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "\n",
    "    distance, indices = faiss_model.search(query_vec, topk)\n",
    "    indices = np.array(indices)\n",
    "    image_paths = [[id2image.get(idx, 'path/not/found') for idx in row] for row in indices]\n",
    "    return image_paths, indices\n",
    "\n",
    "\n",
    "\n",
    "def find_index_in_list(element, my_list):\n",
    "    return my_list.index(element) if element in my_list else 50000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dca4c4-e5ad-44f5-a29b-ad3e28b0b6f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:29.886191Z",
     "start_time": "2024-07-02T12:00:29.880462Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def answer(question,base64_target_image):\n",
    "    client = OpenAI(api_key='')\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"according to the image, answer the question:{question}ï¼ŒYour answer must be direct and simple\",\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_target_image}\",\n",
    "              },\n",
    "            }\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def summary(query,question,answer):\n",
    "    client = OpenAI(api_key='')\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"\n",
    "        Your task is to summarize the information from the image's question and answer and add this information to the original image description.\\\n",
    "        Remember: the summarized information must be concise, and the original description should not be altered.\n",
    "\n",
    "        <question>\n",
    "        {question}\n",
    "        <answer>\n",
    "        {answer}\n",
    "        <image description>\n",
    "        {query}\n",
    "\n",
    "\n",
    "The information extracted from the question and answer should be added to the original description as an attribute or a simple attributive clause.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"\"}\n",
    "      ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6cd7d-141e-4df2-8bf0-d0f3068690c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:39.710422Z",
     "start_time": "2024-07-02T12:00:30.664191Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "faiss_model = faiss.read_index('./checkpoints/blip_faiss.index')\n",
    "with open('./checkpoints/id2image.pickle', 'rb') as f:\n",
    "    id2image = pickle.load(f)\n",
    "    \n",
    "with open('./checkpoints/blip_image_embedding.pickle', 'rb') as f:\n",
    "    image_vector = pickle.load(f)\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab3d4d52-bdd5-4b00-8484-afbc4165fcdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:39.725790Z",
     "start_time": "2024-07-02T12:00:39.711642Z"
    }
   },
   "outputs": [],
   "source": [
    "recall_res = pd.read_csv('recall_res.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c3441b-4070-47c8-8dc2-e77d07a238ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:00:41.787710Z",
     "start_time": "2024-07-02T12:00:41.780991Z"
    }
   },
   "outputs": [],
   "source": [
    "recall_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a70de930a2f16a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LoadPllava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bd19c9a2198b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:01:04.037105Z",
     "start_time": "2024-07-02T12:00:43.991892Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from safetensors import safe_open\n",
    "from pllava import PllavaProcessor, PllavaForConditionalGeneration, PllavaConfig\n",
    "from accelerate import dispatch_model, infer_auto_device_map\n",
    "from accelerate.utils import get_balanced_memory\n",
    "import os\n",
    "\n",
    "def load_pllava(repo_id, num_frames,\n",
    "                use_lora=False, weight_dir=None,\n",
    "                lora_alpha=32, use_multi_gpus=False,\n",
    "                pooling_shape=(16,12,12)):\n",
    "    kwargs = {\n",
    "        'num_frames': num_frames,\n",
    "    }\n",
    "    # print(\"===============>pooling_shape\", pooling_shape)\n",
    "    if num_frames == 0:\n",
    "        kwargs.update(pooling_shape=(0,12,12)) # produce a bug if ever usen the pooling projector\n",
    "    config = PllavaConfig.from_pretrained(\n",
    "        repo_id if not use_lora else weight_dir,\n",
    "        pooling_shape=pooling_shape,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    model = PllavaForConditionalGeneration.from_pretrained(repo_id,\n",
    "                                                               config=config,\n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               )\n",
    "\n",
    "    try:\n",
    "        processor = PllavaProcessor.from_pretrained(repo_id)\n",
    "    except Exception as e:\n",
    "        processor = PllavaProcessor.from_pretrained('llava-hf/llava-1.5-7b-hf')\n",
    "\n",
    "    # config lora\n",
    "    if use_lora and weight_dir is not None:\n",
    "        print(\"Use lora\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, inference_mode=False,  target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            r=128, lora_alpha=lora_alpha, lora_dropout=0.\n",
    "        )\n",
    "        print(\"Lora Scaling:\", lora_alpha/128)\n",
    "        model.language_model = get_peft_model(model.language_model, peft_config)\n",
    "        assert weight_dir is not None, \"pass a folder to your lora weight\"\n",
    "        print(\"Finish use lora\")\n",
    "\n",
    "    # load weights\n",
    "    if weight_dir is not None:\n",
    "        state_dict = {}\n",
    "        save_fnames = os.listdir(weight_dir)\n",
    "        if \"model.safetensors\" in save_fnames:\n",
    "            use_full = False\n",
    "            for fn in save_fnames:\n",
    "                if fn.startswith('model-0'):\n",
    "                    use_full=True\n",
    "                    break\n",
    "        else:\n",
    "            use_full= True\n",
    "\n",
    "        if not use_full:\n",
    "            print(\"Loading weight from\", weight_dir, \"model.safetensors\")\n",
    "            with safe_open(f\"{weight_dir}/model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "                for k in f.keys():\n",
    "                    state_dict[k] = f.get_tensor(k)\n",
    "        else:\n",
    "            print(\"Loading weight from\", weight_dir)\n",
    "            for fn in save_fnames:\n",
    "                if fn.startswith('model-0'):\n",
    "                    with safe_open(f\"{weight_dir}/{fn}\", framework=\"pt\", device=\"cpu\") as f:\n",
    "                        for k in f.keys():\n",
    "                            state_dict[k] = f.get_tensor(k)\n",
    "\n",
    "        if 'model' in state_dict.keys():\n",
    "            with torch.device('meta'): # load large scaler model weight\n",
    "                msg = model.load_state_dict(state_dict['model'], strict=False, assign=True)\n",
    "        else:\n",
    "            with torch.device('meta'):\n",
    "                msg = model.load_state_dict(state_dict, strict=False, assign=True)\n",
    "        print(msg)\n",
    "    # dispatch model weight\n",
    "    if use_multi_gpus:\n",
    "        max_memory = get_balanced_memory(\n",
    "            model,\n",
    "            max_memory=None,\n",
    "            no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "            dtype='bfloat16',\n",
    "            low_zero=False,\n",
    "        )\n",
    "\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            max_memory=max_memory,\n",
    "            no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "            dtype='bfloat16'\n",
    "        )\n",
    "\n",
    "        dispatch_model(model, device_map=device_map)\n",
    "        print(model.hf_device_map)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "# load model\n",
    "llava_model, llava_processor = load_pllava(repo_id='MODELS/pllava-7b', #'llava-hf/llava-1.5-7b-hf',\n",
    "            num_frames=5, # num_images = 5\n",
    "            use_lora=True,\n",
    "            weight_dir='MODELS/pllava-7b',\n",
    "            lora_alpha=4,\n",
    "            use_multi_gpus=False,\n",
    "            pooling_shape=(5,12,12)\n",
    "            )\n",
    "llava_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa4be583e71812f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:01:05.991630Z",
     "start_time": "2024-07-02T12:01:05.982113Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def extract_json(text):\n",
    "    pattern = r'{.*}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'parse incorrectly'\n",
    "\n",
    "def question_attribute_llava(images_path, query, model=None,\n",
    "                     processor=None):\n",
    "    prompt = f\"\"\"You are Pllava, a large vision-language assistant. You are able to understand the video content that the user provides, and assist the user with a variety of tasks using natural language.\n",
    "                 Follow the instructions carefully and explain your answers in detail based on the provided video.\n",
    "                  USER:<image>  USER: You need to find a common object that appears in all 5 pictures but has distinguishing features. Based on this object, ask a question to differentiate the pictures.\n",
    "\n",
    "                Remember, you must ensure the question is specific, not abstract, and the answer should be directly obtainable by looking at the images.\n",
    "\n",
    "                For example:\n",
    "                Example 1: All 5 pictures have people, but the number of people differs. You can ask about the number of people.\n",
    "                Example 2: All 5 pictures have cats, but the colors are different. You can ask about the color.\n",
    "                Example 3: All 5 pictures have traffic lights, but their positions differ. You can ask about the position of the traffic lights.\n",
    "\n",
    "                Ask a specific question based on the object that will help distinguish the pictures. The question is not overlapped with the description: {query}.\n",
    "                Don't ask 2 questions each time. such as what is the attribute of a or b.\n",
    "\n",
    "                Output as the following format\n",
    "                {{\n",
    "                \"Question to differentiate the pictures\":\"\"\n",
    "                }}\n",
    "                \"\"\n",
    "                ASSISTANT:\"\"\"\n",
    "    image_tensor = [load_image(img_file) for img_file in images_path]\n",
    "    inputs = processor(prompt, image_tensor, return_tensors=\"pt\")\n",
    "    inputs = {k:v.to(\"cuda\") for k,v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output_token = model.generate(**inputs, media_type='video',\n",
    "                                    do_sample=False,\n",
    "                                    max_new_tokens=500,\n",
    "                                      num_beams=1,\n",
    "                                      min_length=1,\n",
    "                                    top_p=0.9,\n",
    "                                      repetition_penalty=1,\n",
    "                                      length_penalty=1,\n",
    "                                      temperature=1,\n",
    "                                    ) # dont need to long for the choice.\n",
    "    torch.cuda.empty_cache() # clear the history for this batch\n",
    "    output_text = processor.batch_decode(output_token, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    # extract the question\n",
    "    output_text = output_text.split('ASSISTANT:')[-1].strip()\n",
    "    question_fewshot_json=json.loads(extract_json(output_text))\n",
    "    question = question_fewshot_json[\"Question to differentiate the pictures\"]\n",
    "    return question\n",
    "\n",
    "from collections import defaultdict\n",
    "def downsample_retrieved_images(images_path:list, strategy='clustering', image_vector=None, retrieve_image_indices:list=None):\n",
    "    if strategy == 'clustering':\n",
    "        topk_embedding = image_vector[retrieve_image_indices] # [batch_size, topk, embed_dim]\n",
    "        \n",
    "        kmeans = faiss.Kmeans(d=topk_embedding.shape[1], k=5, niter=10, verbose=False) # k=num_clusters, niter=epoch\n",
    "        # Train the KMeans object\n",
    "        kmeans.train(topk_embedding)\n",
    "        distance, clustering_label = kmeans.index.search(topk_embedding, 1)\n",
    "        \n",
    "        clustering_label = clustering_label.flatten()\n",
    "        label_to_paths = defaultdict(list)\n",
    "        for path, label in zip(images_path, clustering_label):\n",
    "            label_to_paths[label].append(path)\n",
    "        \n",
    "        sampled_paths_list = [random.choice(paths) for paths in label_to_paths.values()]\n",
    "    elif strategy == 'interval':\n",
    "        sampled_paths_list = []\n",
    "        for i in range(0, 50, 10):\n",
    "            sampled_path = random.choice(images_path[i:i+10])\n",
    "            sampled_paths_list.append(sampled_path)\n",
    "    elif strategy == 'topk_cos_similiarity':\n",
    "        sampled_paths_list = images_path\n",
    "    return sampled_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d10bad-51b6-42db-8b0c-eb6ed4beb264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:19:27.056981Z",
     "start_time": "2024-07-02T12:19:10.265744Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "top10=0\n",
    "better=0\n",
    "\n",
    "save_data = []\n",
    "for i in tqdm(range(700)):\n",
    "    query=jsonl_data[i][\"option\"]\n",
    "    # question=jsonl_data[i]['one_tune_questions']\n",
    "    # Generate question using Pllava\n",
    "    images_path, indices = retrieve_topk_images([query],\n",
    "                                             topk=50,\n",
    "                                             faiss_model=faiss_model,\n",
    "                                             blip_model=dialog_encoder,\n",
    "                                             id2image=id2image)\n",
    "    images_path, indices = images_path[0], indices[0] \n",
    "    # Downsample retrieved image\n",
    "    sampled_image_paths = downsample_retrieved_images(images_path,\n",
    "                                                       strategy='interval',\n",
    "                                                       image_vector=image_vector,\n",
    "                                                       retrieve_image_indices=indices\n",
    "                                                      )\n",
    "\n",
    "    question = question_attribute_llava(images_path=sampled_image_paths,\n",
    "                                 query=query,\n",
    "                                 model=llava_model,\n",
    "                                 processor=llava_processor)\n",
    "\n",
    "    target_image_path='./playground/data/css_data/'+recall_res.loc[recall_res['option'] == query, 'target_image'].values[0]\n",
    "    # sampled_image_paths=jsonl_data[i]['downsampled_images_paths']\n",
    "    image_paths,indices=retrieve_topk_images([query],\n",
    "                             topk=10000,\n",
    "                             faiss_model=faiss_model,\n",
    "                             blip_model=dialog_encoder,\n",
    "                             id2image=id2image,\n",
    "                             processor=None, )\n",
    "    image_rank=find_index_in_list(target_image_path , image_paths[0])\n",
    "    # show_image([target_image_path], sentence=None)\n",
    "    # show_image(sampled_image_paths, sentence=None)\n",
    "    \n",
    "    \n",
    "    base64_target_image=encode_image(target_image_path)\n",
    "    answer_of_question=answer(question,base64_target_image)\n",
    "    summary_of_question_and_option=summary(query,question,answer_of_question)\n",
    "    \n",
    "    image_paths_new,indices=retrieve_topk_images([summary_of_question_and_option],\n",
    "                             topk=10000,\n",
    "                             faiss_model=faiss_model,\n",
    "                             blip_model=dialog_encoder,\n",
    "                             id2image=id2image,\n",
    "                             processor=None, )\n",
    "    image_rank_new=find_index_in_list(target_image_path , image_paths_new[0])\n",
    "\n",
    "\n",
    "    is_top10 = is_better = 0\n",
    "    if image_rank_new<=10:\n",
    "        top10+=1\n",
    "        is_top10=1\n",
    "    if image_rank_new<image_rank:\n",
    "        better+=1\n",
    "        is_better=1\n",
    "\n",
    "\n",
    "    record_dict = {'downsampled_images_path': sampled_image_paths,\n",
    "                   'one_tune_questions': question,\n",
    "                   'target_image_path': target_image_path,\n",
    "                   'image_rank': image_rank,\n",
    "                   'option': query,\n",
    "                   'answer_of_question':answer_of_question,\n",
    "                   'summary_of_question_and_option': summary_of_question_and_option,\n",
    "                   'image_rank_new': image_rank_new,\n",
    "                    'is_top10': is_top10,\n",
    "                    'is_better': is_better\n",
    "                   }\n",
    "    save_data.append(record_dict)\n",
    "    with open('./rank_res/all_processes_pllava7b.jsonl', 'a') as f:\n",
    "        f.write(json.dumps(record_dict) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c133a29-e65b-48cd-840d-fadb9f2eeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10,better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182da8e1-03bf-41e9-889e-42c94f718523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:17:52.433957Z",
     "start_time": "2024-07-02T12:17:48.735438Z"
    }
   },
   "outputs": [],
   "source": [
    "i=25\n",
    "query=jsonl_data[i][\"option\"]\n",
    "\n",
    "target_image_path='./playground/data/css_data/'+recall_res.loc[recall_res['option'] == query, 'target_image'].values[0]\n",
    "sampled_image_paths=jsonl_data[i]['downsampled_images_paths']\n",
    "image_paths,indices=retrieve_topk_images([query],\n",
    "                         topk=10000,\n",
    "                         faiss_model=faiss_model,\n",
    "                         blip_model=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         processor=None, )\n",
    "image_rank=find_index_in_list(target_image_path , image_paths[0])\n",
    "show_image([target_image_path], sentence=None)\n",
    "show_image(sampled_image_paths, sentence=None)\n",
    "\n",
    "question=jsonl_data[i]['one_tune_questions']\n",
    "#\n",
    "\n",
    "base64_target_image=encode_image(target_image_path)\n",
    "answer_of_question=answer(question,base64_target_image)\n",
    "summary_of_question_and_option=summary(query,question,answer_of_question)\n",
    "\n",
    "image_paths_new,indices=retrieve_topk_images([summary_of_question_and_option],\n",
    "                         topk=10000,\n",
    "                         faiss_model=faiss_model,\n",
    "                         blip_model=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         processor=None, )\n",
    "image_rank_new=find_index_in_list(target_image_path , image_paths_new[0])\n",
    "\n",
    "if image_rank_new<=10:\n",
    "    top10+=1\n",
    "if image_rank_new<image_rank:\n",
    "    better+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789f1a9-8edf-455d-bad4-5e400e3dce2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T12:18:04.036265Z",
     "start_time": "2024-07-02T12:17:55.882200Z"
    }
   },
   "outputs": [],
   "source": [
    "k=(image_rank+4)//5\n",
    "for i in range(k):\n",
    "    show_image(image_paths[0][i*5:min((i+1)*5,image_rank+1)], sentence=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd7c3e4-cb27-49f4-a09e-ef3aa99bbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=(image_rank_new+4)//5\n",
    "for i in range(k):\n",
    "    show_image(image_paths_new[0][i*5:min((i+1)*5,image_rank_new+1)], sentence=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
