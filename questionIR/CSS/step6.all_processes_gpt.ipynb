{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8913a1ab-36d5-4dae-923e-7daf15a36f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b43c20b-d18b-40d6-989e-b14c8b129a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_image(image_paths, sentence=None):\n",
    "    k=(len(image_paths)+4)//5\n",
    "    fig, axs = plt.subplots(nrows=k, ncols=5, figsize=(20, 8))  \n",
    "    axs = axs.flatten()  \n",
    "\n",
    "    \n",
    "    for ax, img_path in zip(axs, image_paths):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')  \n",
    "            ax.set_title(img_path.split('/')[-1])  \n",
    "        except FileNotFoundError:\n",
    "            ax.imshow(np.zeros((10, 10, 3), dtype=int))  \n",
    "            ax.axis('off')\n",
    "            ax.set_title('File Not Found')\n",
    "    if sentence:\n",
    "        fig.suptitle(sentence, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    model = blip_itm(pretrained='./BLIP/chatir_weights.ckpt',  # Download from Google Drive, see README.md\n",
    "                     med_config='BLIP/configs/med_config.json',\n",
    "                     image_size=224,\n",
    "                     vit='base'\n",
    "                     )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # define Image Embedder (raw_image --> img_feature)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\"\n",
    "                               ).to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text strings.\n",
    "            processor (transformers processor): Processor to tokenize the text.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  \n",
    "        return {'text': text}\n",
    "\n",
    "def encode_text(dataset, model):\n",
    "    \"\"\"CLIP for encode text \"\"\"\n",
    "    # model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=20,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=1,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False,\n",
    "                                              \n",
    "                                              )\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = model(batch['text'])\n",
    "            all_features.append(features.cpu())  \n",
    "    return torch.cat(all_features)  \n",
    "\n",
    "def retrieve_topk_images(query: list,\n",
    "                         topk=10,\n",
    "                         faiss_model=None,\n",
    "                         blip_model=None,\n",
    "                         id2image=None,\n",
    "                         processor=None, ):\n",
    "    text_dataset = TextDataset(query)\n",
    "    query_vec = encode_text(text_dataset, blip_model)\n",
    "    query_vec = query_vec.numpy()\n",
    "    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "    print('query_vec.shape------------', query_vec.shape)\n",
    "    distance, indices = faiss_model.search(query_vec, topk)\n",
    "    indices = np.array(indices)\n",
    "    image_paths = [[id2image.get(idx, 'path/not/found') for idx in row] for row in indices]\n",
    "    return image_paths, indices\n",
    "\n",
    "\n",
    "\n",
    "def find_index_in_list(element, my_list):\n",
    "    return my_list.index(element) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "797f01f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_valid_question(base64_image_top, query, max_retries=5):\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1} to generate a valid question.\")\n",
    "            \n",
    "            question_fewshot = question_attribute(base64_image_top, query)\n",
    "\n",
    "            question_fewshot_json = json.loads(extract_json(question_fewshot))\n",
    "            \n",
    "            if \"Question to differentiate the pictures\" in question_fewshot_json:\n",
    "                return question_fewshot_json[\"Question to differentiate the pictures\"]\n",
    "            else:\n",
    "                print(f\"Key 'Question to differentiate the pictures' not found, retrying...\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"JSONDecodeError: The result is not valid JSON, retrying...\")\n",
    "        except KeyError:\n",
    "            print(\"KeyError: Expected key not found in the generated JSON, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}, retrying...\")\n",
    "\n",
    "    print(\"Failed to generate a valid question after multiple attempts.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621af183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    pattern = r'{.*}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'parse incorrectly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53e0c842-6121-48de-ab9c-cf7b57bfc5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def question_simple(base64_image_list,query):\n",
    "    client = OpenAI(api_key='',base_url = \"\") \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"navigation_llava_4o\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"\"\"\n",
    "              \n",
    "            Your task is to identify the target image based on descriptions provided by users. Due to the ambiguity in user descriptions, you have already searched and found 5 images based on these descriptions. Your task is to analyze the content of these 5 images, ask a question to clarify the user's needs, and your question is not overlapping with the descriptions. Thereby helping the user quickly find the target image. You only need to output one questions within 30 words.\n",
    "            Complete the following tasks step by step:\n",
    "            1. Combine the textual description:, observe these 5 images, and analyze and summarize their common points and differences.\n",
    "            2. To find the target image, ask a question based on these differences that can clarify the user’s needs and help them quickly find the target image.\n",
    "            [Your Question]\n",
    "            Based on the images and sentence description <{query}>, your questions are:\n",
    "              \"\"\",\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[0]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[1]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[2]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[3]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[4]}\",\n",
    "              },\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a43b09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def question_attribute(base64_image_list,query):\n",
    "    client = OpenAI(api_key='',base_url = \"\") \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"navigation_llava_4o\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"\"\"\n",
    "              \n",
    "                You need to find a common object that appears in all 5 pictures but has distinguishing features. Based on this object, ask a question to differentiate the pictures.\n",
    "\n",
    "                Remember, you must ensure the question is specific, not abstract, and the answer should be directly obtainable by looking at the images.\n",
    "                \n",
    "                For example:\n",
    "                Example 1: All 5 pictures have people, but the number of people differs. You can ask about the number of people.\n",
    "                Example 2: All 5 pictures have cats, but the colors are different. You can ask about the color.\n",
    "                Example 3: All 5 pictures have traffic lights, but their positions differ. You can ask about the position of the traffic lights.\n",
    "                \n",
    "                Ask a specific question based on the object that will help distinguish the pictures.\n",
    "                Don't ask 2 questions each time. such as what is the attribute of a or b\n",
    "\n",
    "                Output must follow the format\n",
    "                {{\n",
    "                \"What is the common object that appears in all five pictures\":\"\",\n",
    "                \"What is he distinguishing feature that can help differentiate the picture\":\"\",\n",
    "                \"Questin to differentiate the pictures\":\"\"\n",
    "                }}\n",
    "                \"\"\n",
    "                \n",
    "              \"\"\",\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[0]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[1]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[2]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[3]}\",\n",
    "              },\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image_list[4]}\",\n",
    "              },\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=500,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2da81c0e-5a8d-4a36-aaee-6fdd9b00cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f299623c-0ac1-4adf-9f95-51ba7b95b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer(question,base64_target_image):\n",
    "    client = OpenAI(api_key='',base_url = \"\") \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"navigation_llava_4o\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"according to the image, answer the question:{question}，Your answer must be direct and simple\",\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_target_image}\",\n",
    "              },\n",
    "            }\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95d3c521-5269-4a8b-a83a-cc373c1111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary(query,question,answer):\n",
    "    client = OpenAI(api_key='',base_url = \"\") \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model=\"navigation_llava_4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"\n",
    "        Your task is to summarize the information from the image's question and answer and add this information to the original image description.\\\n",
    "        Remember: the summarized information must be concise, and the original description should not be altered.\n",
    "\n",
    "        <question>\n",
    "        {question}\n",
    "        <answer>\n",
    "        {answer}\n",
    "        <image description>\n",
    "        {query}\n",
    "\n",
    "\n",
    "The information extracted from the question and answer should be added to the original description as an attribute or a simple attributive clause.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"\"}\n",
    "      ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa522eb4-4b95-4f60-8e4e-c165ccdbd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    print(\"Load faiss Model...\")\n",
    "    faiss_model = faiss.read_index('./checkpoints/blip_faiss.index')\n",
    "    print(\"faiss faiss Model Load successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Load id2image Data...\")\n",
    "    with open('./checkpoints/id2image.pickle', 'rb') as f:\n",
    "        id2image = pickle.load(f)\n",
    "    print(\"id2image DataLoad Successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Load id2image Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Load image_vector Data...\")\n",
    "    with open('./checkpoints/blip_image_embedding.pickle', 'rb') as f:\n",
    "        image_vector = pickle.load(f)\n",
    "    print(\"image_vector DataLoad Successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Load image_vector Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd5740de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/.autodl/HYF/questionIR/CSS/MODELS/bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('/root/autodl-tmp/.autodl/HYF/questionIR/CSS/MODELS/bert-base-uncased') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a6d68-432e-4477-b6a9-abb556e94978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_model = faiss.read_index('./checkpoints/blip_faiss.index')\n",
    "with open('./checkpoints/id2image.pickle', 'rb') as f:\n",
    "    id2image = pickle.load(f)\n",
    "    \n",
    "with open('./checkpoints/blip_image_embedding.pickle', 'rb') as f:\n",
    "    image_vector = pickle.load(f)\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acd6b97c-189f-458b-bab9-a0ac5c12b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall_res = pd.read_csv('recall_res.csv')\n",
    "#len(recall_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "560f2a53-3e75-4c5d-b727-be05073fe179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            json_object = json.loads(line.strip())\n",
    "            data.append(json_object)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_csv(file_path):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "file_path = '/root/autodl-tmp/.autodl/HYF/questionIR/CSS/downloads/group_20-50.csv'\n",
    "csv_data = read_csv(file_path)\n",
    "\n",
    "print(csv_data.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bb0d1-b8a5-41e6-80c8-55f7919efc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=30\n",
    "# query=jsonl_data[i][\"option\"]\n",
    "\n",
    "query = csv_data.iloc[i][\"option\"]\n",
    "\n",
    "target_image_path='./playground/data/css_data/'+csv_data.iloc[i][\"target_image\"] \n",
    "show_image([target_image_path], sentence=None)\n",
    "\n",
    "\n",
    "image_paths,indices=retrieve_topk_images([query],\n",
    "                         topk=100,\n",
    "                         faiss_model=faiss_model,\n",
    "                         blip_model=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         processor=None, ) #topk=10000 太多了\n",
    "\n",
    "print(f'target_image_path{target_image_path}, image_paths{image_paths[0]}')\n",
    "\n",
    "image_rank=find_index_in_list(target_image_path , image_paths[0])\n",
    "\n",
    "\n",
    "top_images_path=image_paths[0][:40]\n",
    "random_selection_path = random.sample(top_images_path, 5)\n",
    "base64_image_top=[]\n",
    "for image_path in random_selection_path:\n",
    "    base64_image_top.append(encode_image(image_path))\n",
    "    \n",
    "\n",
    "question = generate_valid_question(base64_image_top, query)\n",
    "\n",
    "\n",
    "base64_target_image=encode_image(target_image_path)\n",
    "answer_of_question=answer(question,base64_target_image)\n",
    "\n",
    "\n",
    "summary_of_question_and_option=summary(query,question,answer_of_question)\n",
    "\n",
    "image_paths_new,indices=retrieve_topk_images([summary_of_question_and_option],\n",
    "                         topk=10000,\n",
    "                         faiss_model=faiss_model,\n",
    "                         blip_model=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         processor=None, )\n",
    "\n",
    "\n",
    "image_rank_new=find_index_in_list(target_image_path , image_paths_new[0])\n",
    "\n",
    "print(query)\n",
    "print(question)\n",
    "print(summary_of_question_and_option)\n",
    "\n",
    "\n",
    "print(\"old_rank:\",image_rank)\n",
    "print(\"new_rank:\",image_rank_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce5290-6ee9-4ab6-a3ba-fc1abcebacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query,question,summary_of_question_and_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ef6ae-9714-47ac-be7a-4581ac26061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(image_paths[0][:image_rank+1], sentence=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b454f-04e4-4182-95cb-d36b6c4e4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_new,indices=retrieve_topk_images([summary_of_question_and_option],\n",
    "                         topk=100, \n",
    "                         faiss_model=faiss_model,\n",
    "                         blip_model=dialog_encoder,\n",
    "                         id2image=id2image,\n",
    "                         processor=None, )  \n",
    "print(f'target_image_path:  {target_image_path}, image_paths:  {image_paths_new[0]}')\n",
    "image_rank_new=find_index_in_list(target_image_path , image_paths_new[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078abf7-4837-4ef2-ba45-f5f49bf252dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_rank_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003bdb9-cb4b-404d-a0bf-89cf942c5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"old_rank:\",image_rank)\n",
    "print(\"new_rank:\",image_rank_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f4a7b-6990-4689-abca-2106085bafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(image_paths_new[0][:image_rank_new+1], sentence=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18480ab8-dc8d-49c3-bf42-8c919f9b6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "query_question=[]\n",
    "rank_storage=[]\n",
    "rank_change=[]\n",
    "\n",
    "\n",
    "top_10_hits_before = 0  \n",
    "top_10_hits_after = 0  \n",
    "total_queries = len(csv_data[:100])  \n",
    "\n",
    "\n",
    "for i in tqdm(range(total_queries)):\n",
    "    query = csv_data[\"option\"][i]\n",
    "    target_image_path = './playground/data/css_data/' + csv_data[\"target_image\"][i]\n",
    "\n",
    "    image_paths, indices = retrieve_topk_images([query],\n",
    "                             topk=50000,\n",
    "                             faiss_model=faiss_model,\n",
    "                             blip_model=dialog_encoder,\n",
    "                             id2image=id2image,\n",
    "                             processor=None)\n",
    "\n",
    "    \n",
    "    image_rank = find_index_in_list(target_image_path, image_paths[0])\n",
    "\n",
    "\n",
    "    if image_rank < 10:  \n",
    "        top_10_hits_before += 1\n",
    "\n",
    "\n",
    "    top_images_path = image_paths[0][:40]\n",
    "    random_selection_path = random.sample(top_images_path, 5)\n",
    "    \n",
    "\n",
    "    base64_image_top = []\n",
    "    for image_path in random_selection_path:\n",
    "        base64_image_top.append(encode_image(image_path))\n",
    "    \n",
    "    question = generate_valid_question(base64_image_top, query)\n",
    "    if question is None:\n",
    "        print(\"Failed to generate a valid question. Skipping this sample.\")\n",
    "        continue\n",
    "    \n",
    "\n",
    "    base64_target_image = encode_image(target_image_path)\n",
    "    answer_of_question = answer(question, base64_target_image)\n",
    "    \n",
    "\n",
    "    summary_of_question_and_option = summary(query, question, answer_of_question)\n",
    "    \n",
    "    \n",
    "    image_paths_new, indices = retrieve_topk_images([summary_of_question_and_option],\n",
    "                             topk=50000,\n",
    "                             faiss_model=faiss_model,\n",
    "                             blip_model=dialog_encoder,\n",
    "                             id2image=id2image,\n",
    "                             processor=None)\n",
    "                             \n",
    "    image_rank_new = find_index_in_list(target_image_path, image_paths_new[0])\n",
    "    rank_storage.append([image_rank, image_rank_new, image_rank - image_rank_new])\n",
    "    rank_change.append(image_rank - image_rank_new)\n",
    "    \n",
    "\n",
    "    if image_rank_new < 10:  \n",
    "        top_10_hits_after += 1\n",
    "\n",
    "\n",
    "top_10_recall_rate_before = top_10_hits_before / total_queries\n",
    "top_10_recall_rate_after = top_10_hits_after / total_queries\n",
    "print(f'top_10_hits: {top_10_hits_before}, total_queries: {total_queries}')\n",
    "print(f\"Top-10 Recall Rate before: {top_10_recall_rate_before * 100:.4f}%\")\n",
    "print(f'---------------------------------------')\n",
    "print(f'top_10_hits: {top_10_hits_after}, total_queries: {total_queries}')\n",
    "print(f\"Top-10 Recall Rate after: {top_10_recall_rate_after * 100:.4f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_questionIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
