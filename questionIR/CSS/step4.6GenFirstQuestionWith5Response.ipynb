{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e345b158-4e50-425e-b1c0-d53f29eb8feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "from accelerate import Accelerator\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39984a-1e43-48ae-a1cf-aba384896f15",
   "metadata": {},
   "source": [
    "### recall images based on the option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d532fcc-987f-4024-bf82-b4f6f68fca3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    model = blip_itm(pretrained='./BLIP/chatir_weights.ckpt',  # Download from Google Drive, see README.md\n",
    "                     med_config='BLIP/configs/med_config.json',\n",
    "                     image_size=224,\n",
    "                     vit='base'\n",
    "                     )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # define Image Embedder (raw_image --> img_feature)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    # define dialog encoder (dialog --> img_feature)\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\"\n",
    "                               ).to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text strings.\n",
    "            processor (transformers processor): Processor to tokenize the text.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        # self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  \n",
    "        return {'text': text}\n",
    "\n",
    "def encode_text(dataset, model):\n",
    "    \"\"\"CLIP for encode text \"\"\"\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=100,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=4,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False,\n",
    "                                              \n",
    "                                              )\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = model(batch['text'])\n",
    "            all_features.append(features.cpu())  \n",
    "    return torch.cat(all_features)  \n",
    "\n",
    "def retrieve_topk_images(query: list,\n",
    "                         topk=10,\n",
    "                         faiss_model=None,\n",
    "                         text_encoder=None,\n",
    "                         id2image=None,\n",
    "                         ):\n",
    "    text_dataset = TextDataset(query)\n",
    "    query_vec = encode_text(text_dataset, text_encoder)\n",
    "    query_vec = query_vec.numpy()\n",
    "    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "\n",
    "    distance, indices = faiss_model.search(query_vec, topk)\n",
    "    indices = np.array(indices)\n",
    "    image_paths = [[id2image.get(idx, 'path/not/found') for idx in row] for row in indices]\n",
    "    return image_paths, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb835d-5bc3-408a-bbf2-35692093a486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faiss_model = faiss.read_index('./checkpoints/blip_faiss.index')\n",
    "with open('./checkpoints/id2image.pickle', 'rb') as f:\n",
    "    id2image = pickle.load(f)\n",
    "    \n",
    "with open('./checkpoints/blip_image_embedding.pickle', 'rb') as f:\n",
    "    image_vector = pickle.load(f)\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfffa6-2062-45bd-9a23-366521162508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "652a4fe2-54cb-4c94-936d-9704c4377e80",
   "metadata": {},
   "source": [
    "### Generate Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0899735-9483-4a9d-8bbe-c13e8524f25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "def image_show(images):\n",
    "\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "\n",
    "\n",
    "    for ax, img in zip(axs.flatten(), images):\n",
    "        ax.imshow(np.array(img))\n",
    "        ax.axis('off')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776f660a-4583-45ee-909f-3d9089657771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from safetensors import safe_open\n",
    "from pllava import PllavaProcessor, PllavaForConditionalGeneration, PllavaConfig\n",
    "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_in_model\n",
    "from accelerate.utils import get_balanced_memory\n",
    "\n",
    "def load_pllava(repo_id, num_frames, use_lora=False, weight_dir=None, lora_alpha=32, use_multi_gpus=False, pooling_shape=(16,12,12)):\n",
    "    kwargs = {\n",
    "        'num_frames': num_frames,\n",
    "    }\n",
    "\n",
    "    if num_frames == 0:\n",
    "        kwargs.update(pooling_shape=(0,12,12)) \n",
    "    config = PllavaConfig.from_pretrained(\n",
    "        repo_id if not use_lora else weight_dir,\n",
    "        pooling_shape=pooling_shape,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model = PllavaForConditionalGeneration.from_pretrained(repo_id, config=config, torch_dtype=torch.bfloat16)\n",
    "        \n",
    "    try:\n",
    "        processor = PllavaProcessor.from_pretrained(repo_id)\n",
    "    except Exception as e:\n",
    "        processor = PllavaProcessor.from_pretrained('llava-hf/llava-1.5-7b-hf')\n",
    "    \n",
    "    processor.padding_side='left'\n",
    "    processor.tokenizer.padding_side='left'\n",
    "    \n",
    "\n",
    "    if use_lora and weight_dir is not None:\n",
    "        print(\"Use lora\")\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, inference_mode=False,  target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            r=128, lora_alpha=lora_alpha, lora_dropout=0.\n",
    "        )\n",
    "        print(\"Lora Scaling:\", lora_alpha/128)\n",
    "        model.language_model = get_peft_model(model.language_model, peft_config)\n",
    "        assert weight_dir is not None, \"pass a folder to your lora weight\"\n",
    "        print(\"Finish use lora\")\n",
    "    \n",
    "    if weight_dir is not None:\n",
    "        state_dict = {}\n",
    "        save_fnames = os.listdir(weight_dir)\n",
    "        if \"model.safetensors\" in save_fnames:\n",
    "            use_full = False\n",
    "            for fn in save_fnames:\n",
    "                if fn.startswith('model-0'):\n",
    "                    use_full=True        \n",
    "                    break\n",
    "        else:\n",
    "            use_full= True\n",
    "\n",
    "        if not use_full:\n",
    "            print(\"Loading weight from\", weight_dir, \"model.safetensors\")\n",
    "            with safe_open(f\"{weight_dir}/model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "                for k in f.keys():\n",
    "                    state_dict[k] = f.get_tensor(k)\n",
    "        else:\n",
    "            print(\"Loading weight from\", weight_dir)\n",
    "            for fn in save_fnames:\n",
    "                if fn.startswith('model-0'):\n",
    "                    with safe_open(f\"{weight_dir}/{fn}\", framework=\"pt\", device=\"cpu\") as f:\n",
    "                        for k in f.keys():\n",
    "                            state_dict[k] = f.get_tensor(k)\n",
    "            \n",
    "        if 'model' in state_dict.keys():\n",
    "            msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "        else:\n",
    "            msg = model.load_state_dict(state_dict, strict=False)\n",
    "        print(msg)\n",
    "\n",
    "    if use_multi_gpus:\n",
    "        max_memory = get_balanced_memory(\n",
    "            model,\n",
    "            max_memory=None,\n",
    "            no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "            dtype='bfloat16',\n",
    "            low_zero=False,\n",
    "        )\n",
    "\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            max_memory=max_memory,\n",
    "            no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "            dtype='bfloat16'\n",
    "        )\n",
    "\n",
    "        dispatch_model(model, device_map=device_map)\n",
    "        print(model.hf_device_map)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    return model, processor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5534e-01a4-40e8-a856-d5626e56a597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, processor = load_pllava(repo_id='MODELS/pllava-7b', \n",
    "            num_frames=5, # num_images = 5\n",
    "            use_lora=True, \n",
    "            weight_dir='MODELS/pllava-7b', \n",
    "            lora_alpha=4, \n",
    "            use_multi_gpus=False, \n",
    "            pooling_shape=(5,12,12),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9513c50-3a27-4f64-b681-2a9477c6f2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d5bd8-cacf-4889-9822-0741728ec9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10fecc5-3203-4826-a78b-01d25cdfa7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def downsample_retrieved_images(images_path:list, strategy='clustering', image_vector=None, retrieve_image_indices:list=None):\n",
    "    if strategy == 'clustering':\n",
    "        topk_embedding = image_vector[retrieve_image_indices] \n",
    "        \n",
    "        kmeans = faiss.Kmeans(d=topk_embedding.shape[1], k=5, niter=10, verbose=False) \n",
    "\n",
    "        kmeans.train(topk_embedding)\n",
    "        distance, clustering_label = kmeans.index.search(topk_embedding, 1)\n",
    "        \n",
    "        clustering_label = clustering_label.flatten()\n",
    "        label_to_paths = defaultdict(list)\n",
    "        for path, label in zip(images_path, clustering_label):\n",
    "            label_to_paths[label].append(path)\n",
    "        \n",
    "        sampled_paths_list = [random.choice(paths) for paths in label_to_paths.values()]\n",
    "    elif strategy == 'interval':\n",
    "        sampled_paths_list = []\n",
    "        for i in range(0, 50, 10):\n",
    "            sampled_path = random.choice(images_path[i:i+10])\n",
    "            sampled_paths_list.append(sampled_path)\n",
    "    elif strategy == 'topk_cos_similiarity':\n",
    "        sampled_paths_list = images_path\n",
    "    return sampled_paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb1027-35be-4687-9fb4-be568d64ac80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import json\n",
    "def extract_json(text):\n",
    "    pattern = r'{.*}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'parse incorrectly'\n",
    "\n",
    "def extract_llm_ouptut(text):\n",
    "    ext_json = extract_json(text)\n",
    "    try:\n",
    "        question = json.loads(ext_json)['Question to differentiate the pictures']\n",
    "    except:\n",
    "        question = ''\n",
    "        pass\n",
    "    return question\n",
    "text = \"\"\"\n",
    "\"What is the common object that appears in all\",\n",
    "\"What is he distinguishing feature that can help differentiate the picture\":\"Color\",\n",
    "\"Question to differentiate the pict\":\"What is the color of the tennis ball in each picture?\"\n",
    "\"\"\"\n",
    "extract_llm_ouptut(text)\n",
    "text = \"\"\"{\n",
    "                    \"What is the common object that appears in all five pictures\": \"a large cake\",\n",
    "                    \"What is the distinguishing feature that can help differentiate the picture\": \"the color of the cake\",\n",
    "                    \"Question to differentiate the pictures\": \"What color is the cake in each of the pictures?\"\n",
    "                   }\"\"\"\n",
    "extract_llm_ouptut(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c25e7-0ca3-427d-a3c6-84392110c83a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d678fd4-716a-40e3-8526-799af914fe40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, option, images_path, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list of dict): List of data dictionaries, each containing images and conversations.\n",
    "            processor: A processor for the model.\n",
    "        \"\"\"\n",
    "        self.option = option\n",
    "        self.images_path = images_path\n",
    "        self.processor = processor\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.option)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_paths = self.images_path[idx] \n",
    "        option = self.option[idx]\n",
    "\n",
    "        image_tensor = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "\n",
    "    \n",
    "        prompt = f\"\"\"You are Pllava, a large vision-language assistant. \n",
    "    # You are able to understand the video content that the user provides, and assist the user with a variety of tasks using natural language.\n",
    "    # Follow the instructions carefully and explain your answers in detail based on the provided video.\n",
    "    #  USER:<image>  USER: You need to find a common object that appears in all 5 pictures but has distinguishing features. Based on this object, ask a question to differentiate the pictures.\n",
    "\n",
    "                Remember, you must ensure the question is specific, not abstract, and the answer should be directly obtainable by looking at the images.\n",
    "                \n",
    "                For example:\n",
    "                Example 1: All 5 pictures have people, but the number of people differs. You can ask about the number of people.\n",
    "                Example 2: All 5 pictures have cats, but the colors are different. You can ask about the color.\n",
    "                Example 3: All 5 pictures have traffic lights, but their positions differ. You can ask about the position of the traffic lights.\n",
    "                \n",
    "                Ask a specific question based on the object that will help distinguish the pictures. The question is not overlapped with the description: {option}.\n",
    "                Don't ask 2 questions each time. such as what is the attribute of a or b\n",
    "\n",
    "                Output as the following format\n",
    "                {{\n",
    "                \"What is the common object that appears in all five pictures\":\"\",\n",
    "                \"What is he distinguishing feature that can help differentiate the picture\":\"\",\n",
    "                \"Question to differentiate the pictures\":\"\"\n",
    "                }}\n",
    "                \"\"\n",
    "                ASSISTANT:\n",
    "                \"\"\"\n",
    "\n",
    "        # print('------The prompt example is that -------\\n', prompt)\n",
    "  \n",
    "        encode = self.processor(prompt, image_tensor, return_tensors=\"pt\")\n",
    "        \n",
    "        return {'input_ids': encode['input_ids'].squeeze(0), # shape: [seq_len]\n",
    "                'attention_mask': encode['attention_mask'].squeeze(0), # shape: [seq_len]\n",
    "                'pixel_values': encode['pixel_values'], # shape: [num_images, 3, 224, 224]\n",
    "                }\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # print(batch.keys())\n",
    "    # input_ids, attention_mask, pixel_values = zip(*batch)\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([item['input_ids'] for item in batch],\n",
    "                                                batch_first=True,\n",
    "                                                padding_value=processor.tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence([item['attention_mask'] for item in batch], batch_first=True, padding_value=0)\n",
    "    pixel_values = torch.cat([item['pixel_values'] for item in batch], dim=0)  # flatten imagesi\n",
    "\n",
    "    return {'input_ids': input_ids, # shape: [batch_size, seq_len]\n",
    "            'attention_mask': attention_mask, # shape: [batch_size, seq_len]\n",
    "            'pixel_values': pixel_values, # shape: [batch_size * num_images, 3, 224, 224]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b7206fd-6200-4796-8ca7-589f79313a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('recall_res.csv')\n",
    "option = data['option'].tolist()\n",
    "# Batch Inference \n",
    "retrieved_images_path, retrieved_indices = retrieve_topk_images(option,\n",
    "                                             topk=100,\n",
    "                                             faiss_model=faiss_model,\n",
    "                                             text_encoder=dialog_encoder,\n",
    "                                             id2image=id2image,\n",
    "                             )\n",
    "\n",
    "# Downsample retrieved image\n",
    "downsampled_images_paths = []\n",
    "for img_path, idx in zip(retrieved_images_path, retrieved_indices):\n",
    "    downsampled_images_path = downsample_retrieved_images(img_path, \n",
    "                                                       strategy='interval', \n",
    "                                                       image_vector=image_vector, \n",
    "                                                       retrieve_image_indices=idx\n",
    "                                                      )\n",
    "    downsampled_images_paths.append(downsampled_images_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d2a28a-3426-4fba-9a1e-00786d16605e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downsampled_images_paths\n",
    "data['interval_sample_images'] = downsampled_images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023a5f2-2c54-4d55-bc35-2a99c1b387dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_repeat_5times = data.loc[data.index.repeat(5)].reset_index(drop=True)\n",
    "data_repeat_5times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62d6e438-e0f3-44b0-b3e5-b46847175df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_repeat_5times.to_csv('./experiment_res/data_repeat_5times.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074c6aad-21bd-40bb-9ab7-a9d2e2375509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "option = data_repeat_5times['option'].tolist()\n",
    "images_path = data_repeat_5times['interval_sample_images'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f7566-4ac6-4295-9029-0b84cb53f77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = MultiModalDataset(option, images_path, processor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn,\n",
    "                                              prefetch_factor=2\n",
    "                                        )\n",
    "model, dataloader = accelerator.prepare(model, dataloader)\n",
    "\n",
    "\n",
    "output_res = []\n",
    "for batch in tqdm(dataloader):\n",
    "    batch = {k:v.to(accelerator.device) for k,v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        output_token = model.generate(**batch, media_type='video',\n",
    "                                do_sample=True,\n",
    "                                max_new_tokens=1000, \n",
    "                                  num_beams=1, \n",
    "                                  min_length=1, \n",
    "                                 top_p=0.9, \n",
    "                                  repetition_penalty=1, \n",
    "                                  length_penalty=1, \n",
    "                                  temperature=0.9,\n",
    "                                )\n",
    "    output_text = processor.batch_decode(output_token, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    for out in output_text:\n",
    "        llm_out = out.split('ASSISTANT:')[-1].strip()\n",
    "        print(llm_out)\n",
    "        processed_text = extract_llm_ouptut(str(llm_out))\n",
    "        output_res.append(processed_text)\n",
    "        print(processed_text)\n",
    "        \n",
    "        save_dict = {\n",
    "            'original_output': llm_out,\n",
    "            'questions': processed_text\n",
    "        }\n",
    "        with open('./experiment_res/one_option_five_images_five_response_13b.jsonl','a') as f:\n",
    "            json_str = json.dumps(save_dict)\n",
    "            f.write(json_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da6865a2-6d8f-45fa-b1ab-9b1899be7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "\n",
    "import json\n",
    "def load_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return [json.loads(l.strip(\"\\n\")) for l in f.readlines()]\n",
    "\n",
    "import re\n",
    "def extract_option(text):\n",
    "    pat_res = re.findall('The question is not overlapped with the description: ([^\\n]+)', text)\n",
    "    if len(pat_res)>0:\n",
    "        return pat_res[0].strip()[:-1]\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47e83f32-bbef-4b22-8200-dcceb0b68f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_res = load_jsonl('./experiment_res/one_option_five_images_five_response_13b.jsonl')\n",
    "# final_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8a2a970-d6d8-404a-8bdf-6394a54f6a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = []\n",
    "questions = []\n",
    "original_outputs = []\n",
    "for i in range(len(final_res)):\n",
    "    options.append(extract_option(final_res[i]['option']))\n",
    "    questions.append(final_res[i]['questions'])\n",
    "    original_outputs.append(final_res[i]['original_output'])\n",
    "response = pd.DataFrame(list(zip(options, questions, original_outputs)), columns=['option', 'one_tune_questions', 'original_output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e3a41-25bf-48e4-8b2b-aa8b9c44c874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response[response['original_output']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e60b438-2fc1-4527-a4a8-b872007200d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repeat_data = pd.read_csv('./experiment_res/data_repeat_5times.csv',)\n",
    "repeat_data.drop_duplicates(subset=['option'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7475a6ff-3f7f-4039-8946-77be064ff308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repeat_data = repeat_data[['option','target_image']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "183b8aea-4e21-4421-80aa-885eb46f0939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data= pd.merge(response, repeat_data, how='left', on=['option'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7ee4384-742b-4ead-ae17-5434a6703839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data = merged_data[merged_data['one_tune_questions']!='']\n",
    "merged_data = merged_data.drop_duplicates(subset=['option','one_tune_questions'])\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "merged_data.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7a79c-d98f-4e9a-b16b-825cd77861a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "31147a9c-8c52-49fe-89ed-a90a8db196fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data.to_csv('./rank_res/one_option_five_images_five_response_13b.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
