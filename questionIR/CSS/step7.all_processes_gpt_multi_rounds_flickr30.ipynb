{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import faiss \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import pickle \n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import json\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_image(image_paths, sentence=None):\n",
    "\n",
    "    \n",
    "    k=(len(image_paths)+4)//5 \n",
    "    fig, axs = plt.subplots(nrows=k, ncols=5, figsize=(20, 8))  \n",
    "    axs = axs.flatten()  \n",
    "\n",
    "    \n",
    "    for ax, img_path in zip(axs, image_paths):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')  \n",
    "            ax.set_title(img_path.split('/')[-1])  \n",
    "        except FileNotFoundError:\n",
    "            ax.imshow(np.zeros((10, 10, 3), dtype=int))  \n",
    "            ax.axis('off')\n",
    "            ax.set_title('File Not Found')\n",
    "    if sentence:\n",
    "        fig.suptitle(sentence, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class ImageEmbedder:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        \"\"\" model projects image to vector, processor load and prepare image to the model\"\"\"\n",
    "        self.model = model\n",
    "        self.processor = preprocessor\n",
    "\n",
    "def BLIP_BASELINE():\n",
    "    from torchvision import transforms\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, './BLIP')\n",
    "    from BLIP.models.blip_itm import blip_itm\n",
    "    # load model\n",
    "    model = blip_itm(pretrained='./BLIP/chatir_weights.ckpt',  # Download from Google Drive, see README.md\n",
    "                     med_config='BLIP/configs/med_config.json',\n",
    "                     image_size=224,\n",
    "                     vit='base'\n",
    "                     )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # define Image Embedder (raw_image --> img_feature)\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])\n",
    "\n",
    "    def blip_project_img(image):\n",
    "        embeds = model.visual_encoder(image)\n",
    "        projection = model.vision_proj(embeds[:, 0, :])\n",
    "        return F.normalize(projection, dim=-1)\n",
    "\n",
    "    def blip_prep_image(path):\n",
    "        raw = Image.open(path).convert('RGB')\n",
    "        return transform_test(raw)\n",
    "\n",
    "    image_embedder = ImageEmbedder(blip_project_img, lambda path: blip_prep_image(path))\n",
    "\n",
    "    # define dialog encoder (dialog --> img_feature)\n",
    "    def dialog_encoder(dialog):\n",
    "        text = model.tokenizer(dialog, padding='longest', truncation=True,\n",
    "                               max_length=200,\n",
    "                               return_tensors=\"pt\"\n",
    "                               ).to(device)\n",
    "\n",
    "        text_output = model.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
    "                                         return_dict=True, mode='text')\n",
    "\n",
    "        shift = model.text_proj(text_output.last_hidden_state[:, 0, :])\n",
    "        return F.normalize(shift, dim=-1)\n",
    "\n",
    "    return dialog_encoder, image_embedder\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts: list):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts (list of str): List of text strings.\n",
    "            processor (transformers processor): Processor to tokenize the text.\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        # self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]  # Get the text at the provided index\n",
    "        return {'text': text}\n",
    "\n",
    "def encode_text(dataset, model):\n",
    "    \"\"\"CLIP for encode text \"\"\"\n",
    "    # model.eval()\n",
    "    data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                              batch_size=20,\n",
    "                                              pin_memory=True,\n",
    "                                              num_workers=1,\n",
    "                                              prefetch_factor=2,\n",
    "                                              shuffle=False,\n",
    "                                              \n",
    "                                              )\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = model(batch['text'])\n",
    "            all_features.append(features.cpu())  \n",
    "    return torch.cat(all_features)  \n",
    "\n",
    "\n",
    "\n",
    "def retrieve_topk_images(query: list,\n",
    "                         topk=10,\n",
    "                         faiss_model=None,\n",
    "                         blip_model=None,\n",
    "                         id2image=None,\n",
    "                         processor=None, ):\n",
    "    text_dataset = TextDataset(query)\n",
    "    query_vec = encode_text(text_dataset, blip_model)\n",
    "    query_vec = query_vec.numpy()\n",
    "    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)\n",
    "    # print(f\"faiss_model vector dimension (d): {faiss_model.d}\")\n",
    "    # print(f\"Query vector dimension: {query_vec.shape}\")\n",
    "\n",
    "    distance, indices = faiss_model.search(query_vec, topk)\n",
    "    # print(f\"Indices returned by faiss_model: {indices[:10]}\")  \n",
    "    indices = np.array(indices)\n",
    "    image_paths = [[id2image.get(idx, 'path/not/found') for idx in row] for row in indices]\n",
    "    \n",
    "    \n",
    "    # print(f\"Image paths for first 100 indices: {image_paths[0][:100]}\")\n",
    "    return image_paths, indices\n",
    "\n",
    "\n",
    "\n",
    "def find_index_in_list(element, my_list):\n",
    "    return my_list.index(element) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset class for the corpus images (the 50k potential candidates)\"\"\"\n",
    "    def __init__(self, image_paths, preprocessor):\n",
    "        self.image_paths = image_paths\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = self.preprocessor(image_path)  \n",
    "        return {'id': idx, 'image': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def encode_images(dataset, image_embedder):\n",
    "    # model.eval()\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=500,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=2,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=False,\n",
    "                                             prefetch_factor=2\n",
    "                                             )\n",
    "    print(\"Preparing corpus (search space)...\")\n",
    "    corpus_vectors = []\n",
    "    # corpus_ids = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch_vectors = F.normalize(image_embedder.model(batch['image'].to(device)), dim=-1)\n",
    "            corpus_vectors.append(batch_vectors)\n",
    "            # corpus_ids.append(batch['id'].to(device))\n",
    "\n",
    "        corpus_vectors = torch.cat(corpus_vectors)\n",
    "    return corpus_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()\n",
    "\n",
    "image_fold = './playground/data/flickr30k-images'\n",
    "image_paths = os.listdir(image_fold)#[:20]\n",
    "merge_image_path = [os.path.join(image_fold, p) for p in image_paths]\n",
    "merge_image_path = sorted(merge_image_path)\n",
    "id2image = dict(zip(range(len(merge_image_path)), merge_image_path))\n",
    "with open('./checkpoints/id2image_flickr30k_BLIP_FT.pickle', 'wb') as f:\n",
    "    pickle.dump(id2image, f)\n",
    "\n",
    "dataset = ImageDataset(merge_image_path, image_embedder.processor)\n",
    "\n",
    "encoded_images = encode_images(dataset, image_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = encoded_images.cpu().numpy()\n",
    "encoded_images /= np.linalg.norm(encoded_images, axis=1, keepdims=True) \n",
    "\n",
    "\n",
    "dimension = encoded_images.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dimension) # \n",
    "faiss_index.add(encoded_images)\n",
    "faiss.write_index(faiss_index,'./checkpoints/blip_faiss_flickr30k_BLIP_FT.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checkpoints/blip_image_embedding_flickr30k_BLIP_FT.pickle', 'wb') as f:\n",
    "    pickle.dump(encoded_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_valid_question(base64_image_top, system_prompt, previous_questions, max_retries=5):\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # print(f\"Attempt {attempt + 1} to generate a valid question.\")\n",
    "\n",
    "            \n",
    "            question_fewshot = question_attribute(base64_image_top, system_prompt, previous_questions)\n",
    "\n",
    "            \n",
    "            question_fewshot_json = extract_json(question_fewshot)\n",
    "\n",
    "            if question_fewshot_json:\n",
    "                question_fewshot_json = json.loads(question_fewshot_json)  \n",
    "                if \"Question to differentiate the pictures\" in question_fewshot_json:\n",
    "                    return question_fewshot_json[\"Question to differentiate the pictures\"]\n",
    "                else:\n",
    "                    print(f\"Key 'Question to differentiate the pictures' not found, retrying...\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"JSONDecodeError: The result is not valid JSON, retrying...\")\n",
    "        except KeyError:\n",
    "            print(\"KeyError: Expected key not found in the generated JSON, retrying...\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}, retrying...\")\n",
    "        \n",
    "        \n",
    "        attempt += 1\n",
    "\n",
    "\n",
    "    print(\"Failed to generate a valid question after multiple attempts.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    pattern = r'{.*}'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return 'parse incorrectly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer(question,base64_target_image):\n",
    "    client = OpenAI(api_key=,base_url = \"\") \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "      model=\"\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"according to the image, answer the question:{question}ï¼ŒYour answer must be direct and simple\",\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_target_image}\",\n",
    "              },\n",
    "            }\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=100,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an assistant that helps users to differentiate between images by generating specific, non-repetitive questions based on given images. \n",
    "Given 5 pictures and a list of questions you have previously asked, your task is to find common objects that appear in all 5 pictures but have distinguishing features. Based on these objects, ask a question that is not included in [previous questions] to differentiate the pictures.\n",
    "Make sure the question is specific, directly related to the images, and not abstract. Avoid repeating the same attributes mentioned in any previous questions. And don't ask two questions at the same time, such as 'What is the attribute of a or b?'.\n",
    "\n",
    "\n",
    "For example:\n",
    "Example 1: All 5 pictures have people, but the number of people differs. You can ask about the number of people.\n",
    "Example 2: All 5 pictures have cats, but the colors are different. You can ask about the color.\n",
    "Example 3: All 5 pictures have traffic lights, but their positions differ. You can ask about the position of the traffic lights.\n",
    "\n",
    "\n",
    "ensure that the output strictly(have to !) follows this JSON format:\n",
    "{\n",
    "\"What is the common object that appears in all five pictures\":\"Answer1\",\n",
    "\"What is the distinguishing feature that can help differentiate the picture\":\"Answer2\",\n",
    "\"Question to differentiate the pictures\":\"Answer3\"\n",
    "}\n",
    "The output should not contain any extra characters or text outside of this JSON structure.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_attribute(base64_image_list, system_prompt, previous_questions):\n",
    "    client = OpenAI(api_key=, base_url=\"\")  \n",
    "\n",
    "    \n",
    "    previous_questions_text = \" \".join([f'Previous Question: {q}' for q in previous_questions])\n",
    "    # print(\"Previous_Question:\",previous_questions)\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    [Previous questions]: {previous_questions_text}.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},  \n",
    "        {\"role\": \"user\",\"content\": [{\"type\": \"text\", \"text\": user_prompt}]},  \n",
    "      ]\n",
    "        \n",
    "    for img_base64 in base64_image_list:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}\n",
    "            }]\n",
    "        })\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"\",\n",
    "      messages=messages,\n",
    "      max_tokens=500,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary(query,question,answer):\n",
    "    client = OpenAI(api_key=,base_url = \"\") \n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model=\"\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"\n",
    "        Your task is to summarize the information from the image's question and answer and add this information to the original image description.\\\n",
    "        Remember: the summarized information must be concise, and the original description should not be altered.\n",
    "\n",
    "        <question>\n",
    "        {question}\n",
    "        <answer>\n",
    "        {answer}\n",
    "        <image description>\n",
    "        {query}\n",
    "\n",
    "\n",
    "The information extracted from the question and answer should be added to the original description as an attribute or a simple attributive clause.\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"\"}\n",
    "      ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    print(\"Load faiss Model...\")\n",
    "    faiss_model = faiss.read_index('./checkpoints/blip_faiss_flickr30k_BLIP_FT.index')\n",
    "    print(\"faiss faiss Model Load successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Load id2image Data...\")\n",
    "    with open('./checkpoints/id2image_flickr30k_BLIP_FT.pickle', 'rb') as f:\n",
    "        id2image = pickle.load(f)\n",
    "    print(\"id2image DataLoad Successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Load id2image Error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"Load image_vector Data...\")\n",
    "    with open('./checkpoints/blip_image_embedding_flickr30k_BLIP_FT.pickle', 'rb') as f:\n",
    "        image_vector = pickle.load(f)\n",
    "    print(\"image_vector DataLoad Successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Load image_vector Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('/root/autodl-tmp/.autodl/HYF/questionIR/CSS/MODELS/bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('/root/autodl-tmp/.autodl/HYF/questionIR/CSS/MODELS/bert-base-uncased') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_model = faiss.read_index('./checkpoints/blip_faiss_flickr30k_BLIP_FT.index')\n",
    "with open('./checkpoints/id2image_flickr30k_BLIP_FT.pickle', 'rb') as f:\n",
    "    id2image = pickle.load(f)\n",
    "    \n",
    "with open('./checkpoints/blip_image_embedding_flickr30k_BLIP_FT.pickle', 'rb') as f:\n",
    "    image_vector = pickle.load(f)\n",
    "dialog_encoder, image_embedder = BLIP_BASELINE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader:\n",
    "    def load_data(self):\n",
    "        raise NotImplementedError(\"This method should be covered by subclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CSVDataLoader(DataLoader):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        return pd.read_csv(self.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "class HuggingFaceDataLoader(DataLoader):\n",
    "    def __init__(self, dataset_name, split):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "\n",
    "    def load_data(self):\n",
    "        dataset = load_dataset(self.dataset_name, split=self.split)\n",
    "        return dataset.to_pandas()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loader(file_type, file_path=None, dataset_name=None, split=None):\n",
    "    if file_type == \"csv\":\n",
    "        return CSVDataLoader(file_path)\n",
    "    elif file_type == \"huggingface\":\n",
    "        return HuggingFaceDataLoader(dataset_name, split)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "def read_csv(file_path):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "file_path = '/root/autodl-tmp/.autodl/HYF/questionIR/CSS/downloads/group_20-50.csv'\n",
    "csv_data = read_csv(file_path)\n",
    "\n",
    "print(csv_data.shape)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_sample_case(file_type, file_path=None, dataset_name=None, split=None):\n",
    "    \n",
    "    data_loader = get_data_loader(file_type, file_path=file_path, dataset_name=dataset_name, split=split)\n",
    "    data = data_loader.load_data()  # LoadData\n",
    "\n",
    "    print(data.shape)  \n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    print(data.iloc[i])\n",
    "    \n",
    "    raw_str = data.iloc[i][\"raw\"]\n",
    "    raw_list = json.loads(raw_str)  \n",
    "    query = raw_list[0]  \n",
    "    print(f'query: {query}')\n",
    "    target_image_path = './playground/data/flickr30k-images/' + data.iloc[i][\"filename\"]  \n",
    "    \n",
    "    \n",
    "    image_paths, indices = retrieve_topk_images([query], topk=50000, faiss_model=faiss_model, blip_model=dialog_encoder, id2image=id2image, processor=None)\n",
    "\n",
    "    image_rank = find_index_in_list(target_image_path, image_paths[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    initial_rank = image_rank\n",
    "    max_rounds = 5\n",
    "    rank_threshold = 10\n",
    "    round_number = 0\n",
    "    accumulated_summary = \"\"\n",
    "    previous_questions = [query]\n",
    "    image_ranks_per_round = []\n",
    "    \n",
    "    \n",
    "    query_with_feedback = \"Initial Query: \" + query + accumulated_summary  \n",
    "\n",
    "\n",
    "    \n",
    "    while round_number < max_rounds:\n",
    "        round_number += 1\n",
    "        print(f\"Round {round_number}:\")\n",
    "        \n",
    "        \n",
    "        top_images_path = image_paths[0][:40]\n",
    "        random_selection_path = random.sample(top_images_path, 5)\n",
    "        base64_image_top = [encode_image(image_path) for image_path in random_selection_path]\n",
    "        \n",
    "        \n",
    "        question = generate_valid_question(base64_image_top, system_prompt, previous_questions)\n",
    "        previous_questions.append(question)\n",
    "        \n",
    "        \n",
    "        base64_target_image = encode_image(target_image_path)\n",
    "        answer_of_question = answer(question, base64_target_image)\n",
    "        \n",
    "        print(f'new -> question: {question}, answer_of_question: {answer_of_question}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        accumulated_summary = f\" Question: {question} Answer: {answer_of_question}\"\n",
    "        \n",
    "        \n",
    "        query_with_feedback += accumulated_summary  \n",
    "        print(f'query_with_feedback: {query_with_feedback}, ')\n",
    "        \n",
    "        summary_of_question_and_option = summary(query_with_feedback, question, answer_of_question)\n",
    "        image_paths_new, indices = retrieve_topk_images([summary_of_question_and_option], topk=50000, faiss_model=faiss_model, blip_model=dialog_encoder, id2image=id2image, processor=None)\n",
    "        \n",
    "        \n",
    "        image_rank_new = find_index_in_list(target_image_path, image_paths_new[0])\n",
    "        image_ranks_per_round.append((image_rank_new, image_paths_new[0]))  \n",
    "        \n",
    "        \n",
    "        print(f\"Old Rank: {image_rank}, New Rank: {image_rank_new}\")\n",
    "        \n",
    "        \n",
    "        if image_rank_new < rank_threshold:\n",
    "            print(f\"Image rank ({image_rank_new}) is below the threshold ({rank_threshold}). Ending early.\")\n",
    "\n",
    "            break\n",
    "        \n",
    "        \n",
    "        image_paths = image_paths_new\n",
    "        image_rank = image_rank_new\n",
    "\n",
    "    \n",
    "    best_rank, best_image_paths = min(image_ranks_per_round, key=lambda x: x[0])  \n",
    "    best_image_path = best_image_paths[0]  \n",
    "\n",
    "    \n",
    "    if best_rank < initial_rank:\n",
    "        print(f\"Final Best Rank improved from {initial_rank} to {best_rank}, Best Image Path: {best_image_path}\")\n",
    "    else:\n",
    "        print(f\"No significant improvement. Final Best Rank: {best_rank}, Best Image Path: {best_image_path}\")\n",
    "        \n",
    "    show_image(image_paths[0][:best_rank+1], sentence=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "run_sample_case(file_type=\"csv\", file_path=\"/root/autodl-tmp/.autodl/HYF/questionIR/CSS/downloads/flickr30k/flickr_annotations_filtered_30k.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = '/root/autodl-tmp/.autodl/HYF/questionIR/CSS/downloads/flickr30k/'\n",
    "file_name_list = [\n",
    "    'flickr_annotations_filtered_30k.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_csv_file(output_file):\n",
    "    fieldnames = [\n",
    "        'file_name', \n",
    "        'query_number', \n",
    "        'initial_rank', \n",
    "        'best_rank', \n",
    "        'rounds', \n",
    "        'image_ranks_per_round', \n",
    "        'top_10_hits_before', \n",
    "        'top_10_hits_after',\n",
    "        'queries_feedback',  \n",
    "        'average_rank'\n",
    "    ]\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "    return fieldnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results_to_csv(output_file, fieldnames, file_name, query_number, initial_rank, best_rank, rounds, image_ranks_per_round, top_10_hits_before, top_10_hits_after, queries_feedback):\n",
    "    \n",
    "\n",
    "    ranks = list(map(int, image_ranks_per_round.split(',')))  \n",
    "    average_rank = sum(ranks) / len(ranks) if ranks else 0\n",
    "    \n",
    "    with open(output_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({\n",
    "            'file_name': file_name,\n",
    "            'query_number': query_number,\n",
    "            'initial_rank': initial_rank,\n",
    "            'best_rank': best_rank,\n",
    "            'rounds': rounds,\n",
    "            'image_ranks_per_round': image_ranks_per_round,\n",
    "            'top_10_hits_before': top_10_hits_before,\n",
    "            'top_10_hits_after': top_10_hits_after,\n",
    "            'queries_feedback': queries_feedback,  \n",
    "            'average_rank': average_rank  \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_csv(file_path):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "query_question = []\n",
    "\n",
    "\n",
    "max_rounds = 5\n",
    "rank_threshold = 10  \n",
    "\n",
    "\n",
    "top_10_hits_before = 0  \n",
    "top_10_hits_after = 0  \n",
    "total_queries_accumulated = 0  \n",
    "\n",
    "\n",
    "output_file = 'multi-round-result_Blip_FT_flickr30k.csv'\n",
    "\n",
    "\n",
    "fieldnames = init_csv_file(output_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file_name in file_name_list:\n",
    "    file_path = folder_path + file_name\n",
    "    csv_data = read_csv(file_path)  \n",
    "\n",
    "    total_queries = len(csv_data)  \n",
    "    total_queries_accumulated += total_queries  \n",
    "    '''\n",
    "    \n",
    "    start_row = max(0, start_row)\n",
    "    end_row = min(total_queries, end_row)\n",
    "    '''\n",
    "\n",
    "    top_10_hits_before_dataset = 0  \n",
    "    top_10_hits_after_dataset = 0  \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for i in tqdm(range(total_queries)):\n",
    "\n",
    "        raw_str = csv_data.iloc[i][\"raw\"]\n",
    "        raw_list = json.loads(raw_str)  \n",
    "        query = raw_list[0]  \n",
    "        \n",
    "        target_image_path = './playground/data/flickr30k-images/' + csv_data[\"filename\"][i]  \n",
    "        \n",
    "\n",
    "        image_ranks_per_round = []\n",
    "        queries_feedback = []  \n",
    "        \n",
    "\n",
    "        image_paths, indices = retrieve_topk_images([query],\n",
    "                                                    topk=50000,\n",
    "                                                    faiss_model=faiss_model,\n",
    "                                                    blip_model=dialog_encoder,\n",
    "                                                    id2image=id2image,\n",
    "                                                    processor=None)\n",
    "        image_rank = find_index_in_list(target_image_path, image_paths[0])\n",
    "        \n",
    "\n",
    "        if image_rank < 10:\n",
    "            top_10_hits_before += 1\n",
    "            top_10_hits_before_dataset += 1\n",
    "\n",
    "\n",
    "        initial_rank = image_rank\n",
    "        \n",
    "\n",
    "        accumulated_summary = \"\"\n",
    "        previous_questions = [query]  \n",
    "        query_with_feedback = \"Initial Query: \" + query + accumulated_summary  \n",
    "        \n",
    "        round_number = 0\n",
    "        \n",
    "        \n",
    "        while round_number < max_rounds:\n",
    "            round_number += 1\n",
    "\n",
    "            top_images_path = image_paths[0][:40]\n",
    "            random_selection_path = random.sample(top_images_path, 5)\n",
    "            base64_image_top = [encode_image(image_path) for image_path in random_selection_path]\n",
    "            \n",
    "\n",
    "            question = generate_valid_question(base64_image_top, system_prompt, previous_questions)\n",
    "            \n",
    "            \n",
    "            if question is None:\n",
    "                print(f\"Failed to generate a valid question for image {i+1}, skipping this sample.\")\n",
    "                continue\n",
    "            \n",
    "            query_question.append([query_with_feedback, question])\n",
    "            previous_questions.append(question)  \n",
    "            \n",
    "\n",
    "            base64_target_image = encode_image(target_image_path)\n",
    "            answer_of_question = answer(question, base64_target_image)\n",
    "            \n",
    "\n",
    "            accumulated_summary = f\" Question: {question} Answer: {answer_of_question}\"\n",
    "            \n",
    "            query_with_feedback += accumulated_summary  \n",
    "            \n",
    "            \n",
    "            summary_of_question_and_option = summary(query_with_feedback, question, answer_of_question)\n",
    "            \n",
    "            \n",
    "            image_paths_new, indices = retrieve_topk_images([summary_of_question_and_option],\n",
    "                                                            topk=50000,\n",
    "                                                            faiss_model=faiss_model,\n",
    "                                                            blip_model=dialog_encoder,\n",
    "                                                            id2image=id2image,\n",
    "                                                            processor=None)\n",
    "            \n",
    "\n",
    "            image_rank_new = find_index_in_list(target_image_path, image_paths_new[0])\n",
    "            image_ranks_per_round.append(image_rank_new)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            # print(f\"Round {round_number + 1}: New Rank: {image_rank_new}, Old Rank: {image_rank},\")\n",
    "            \n",
    "            \n",
    "            if image_rank_new < rank_threshold:\n",
    "                print(f\"Image rank ({image_rank_new}) is below the threshold ({rank_threshold}). Ending early.\")\n",
    "                break  \n",
    "            \n",
    "\n",
    "            image_paths = image_paths_new\n",
    "            \n",
    "        \n",
    "        queries_feedback.append({\n",
    "            'query_with_feedback': query_with_feedback,\n",
    "            # 'question': question,\n",
    "            # 'answer': answer_of_question\n",
    "        }) \n",
    "\n",
    "        best_rank = min(image_ranks_per_round)  \n",
    "            \n",
    "        \n",
    "        if best_rank < 10:\n",
    "            top_10_hits_after += 1\n",
    "            top_10_hits_after_dataset += 1\n",
    "\n",
    "        \n",
    "        save_results_to_csv(\n",
    "            output_file, \n",
    "            fieldnames, \n",
    "            file_name, \n",
    "            i + 1, \n",
    "            initial_rank, \n",
    "            best_rank, \n",
    "            round_number , \n",
    "            ','.join(map(str, image_ranks_per_round)), \n",
    "            top_10_hits_before_dataset, \n",
    "            top_10_hits_after_dataset,\n",
    "            queries_feedback  \n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    top_10_recall_rate_before_dataset = top_10_hits_before_dataset / total_queries\n",
    "    top_10_recall_rate_after_dataset = top_10_hits_after_dataset / total_queries\n",
    "    print(f'Dataset: {file_name} - Top-10 hits before multi-round: {top_10_hits_before_dataset}, total_queries: {total_queries}')\n",
    "    print(f\"Dataset: {file_name} - Top-10 Recall Rate before multi-round: {top_10_recall_rate_before_dataset * 100:.4f}%\")\n",
    "    print(f'Dataset: {file_name} - Top-10 hits after multi-round: {top_10_hits_after_dataset}, total_queries: {total_queries}')\n",
    "    print(f\"Dataset: {file_name} - Top-10 Recall Rate after multi-round: {top_10_recall_rate_after_dataset * 100:.4f}%\")\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "\n",
    "top_10_recall_rate_before = top_10_hits_before / total_queries_accumulated\n",
    "top_10_recall_rate_after = top_10_hits_after / total_queries_accumulated\n",
    "print(f'top_10_hits before multi-round: {top_10_hits_before}, total_queries_accumulated: {total_queries_accumulated}')\n",
    "print(f\"Top-10 Recall Rate before multi-round: {top_10_recall_rate_before * 100:.4f}%\")\n",
    "print(f'top_10_hits after multi-round: {top_10_hits_after}, total_queries_accumulated: {total_queries_accumulated}')\n",
    "print(f\"Top-10 Recall Rate after multi-round: {top_10_recall_rate_after * 100:.4f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_questionIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
